{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "1. Extract/organize line configuration and conduit configuration data from old_exhibit\n",
    "2. Write pdf_comparison class\n",
    "3. Write pdf_markup class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (20231228)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfminer.six) (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (0.11.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfplumber) (10.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: numpy in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: textdistance in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (4.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: regex in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (2024.5.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (6.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfminer.six\n",
    "%pip install pdfplumber\n",
    "%pip install numpy pandas\n",
    "%pip install textdistance\n",
    "%pip install regex\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipympl\n",
    "%pip install reportlab>=3.6.2\n",
    "%pip install PyPDF2\n",
    "%pip install ocrmypdf\n",
    "%pip install pdf2jpg\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages, extract_text\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTRect, LTFigure\n",
    "# To extract text from tables in PDF\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Iterable, Dict, Tuple\n",
    "import regex\n",
    "from textdistance import hamming, jaro, levenshtein\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "os.environ['USE_TORCH'] = '1'\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfMerger\n",
    "from ocrmypdf.hocrtransform import HocrTransform\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdf_data():\n",
    "    def __init__(self,   \n",
    "                 orig_filepath: str | Path, \n",
    "                 ocr_filepath:  str | Path, \n",
    "                 config:        str | Path,\n",
    "                 key_val_sep:   str = ':' ,) -> None:\n",
    "        self.orig_filepath = orig_filepath\n",
    "        self.ocr_filepath = ocr_filepath\n",
    "        self.config = config  \n",
    "        self.text_df = None  \n",
    "        self.px_col_sep = 8\n",
    "        self.px_word_sep = 2\n",
    "        self.col_sep_str = ' | '\n",
    "        self.key_val_sep = key_val_sep\n",
    "\n",
    "    def combine_key_value_pairs_in_words_df(self, words_df):\n",
    "        drop_idxs = list()\n",
    "        for wIdx, word in words_df.iterrows():\n",
    "            if 'source' in words_df.columns and any(words_df['source'] == 'ocr'):\n",
    "                test = (word['text'][-1] == self.key_val_sep) and \\\n",
    "                       (wIdx != words_df.index[-1])  \n",
    "            else:\n",
    "                test = (word['text'][-1] == self.key_val_sep) and \\\n",
    "                       (wIdx != words_df.index[-1]) and \\\n",
    "                       (words_df.loc[wIdx+1, 'top'] == word['top'])  \n",
    "\n",
    "            if test:\n",
    "                drop_idxs.append(wIdx+1)\n",
    "                words_df.loc[wIdx, 'text'] = f\"{word['text']}{words_df.loc[wIdx+1, 'text']}\"  \n",
    "                words_df.loc[wIdx, 'right'] = words_df.loc[wIdx+1, 'right']       \n",
    "        words_df = words_df.drop(drop_idxs).reset_index(drop=True)    \n",
    "\n",
    "        return words_df\n",
    "\n",
    "    def fill_implicit_keys(self, section_header, left_mult=2, right_mult=2):\n",
    "        section_dict = self.sections[section_header]\n",
    "        for sub_idx, subsection_bounds in section_dict['bounds'].iterrows():    \n",
    "            mask = (self.text_df['norm_top'   ] > subsection_bounds['top'   ]) & \\\n",
    "                   (self.text_df['norm_bottom'] < subsection_bounds['bottom']) \n",
    "            subsect_df = self.text_df.loc[mask, :]\n",
    "\n",
    "            is_ocr = True if any(subsect_df['source'] == 'ocr') else False\n",
    "            if is_ocr:\n",
    "                # page = pdfplumber.open(self.ocr_filepath ).pages[subsect_df.loc[subsect_df.index[0], 'page']]\n",
    "                words_df = self.ocr_words_df.copy()\n",
    "                words_mask = (words_df['norm_top'   ] > subsection_bounds['top'   ]) & \\\n",
    "                             (words_df['norm_bottom'] < subsection_bounds['bottom']) \n",
    "                words_df = words_df.loc[words_mask, :]\n",
    "                merged_rows = [idx for idx, word in words_df.iterrows() if ' ' in word['text']]\n",
    "                for idx in merged_rows:\n",
    "                    words_df.loc[idx, 'text'] = words_df.loc[idx, 'text'].split(' ')[0] \n",
    "                words_df['top'] = round(words_df['top'], -1)\n",
    "            else:\n",
    "                page = pdfplumber.open(self.orig_filepath).pages[subsect_df.loc[subsect_df.index[0], 'page']]\n",
    "                words_df = self.get_words_df(page, subsect_df['top'].min(), subsect_df['bottom'].max())\n",
    "                words_df.loc[:, 'top'] = np.round(words_df['top']) \n",
    "            \n",
    "\n",
    "            words_df.sort_values(by=['top', 'left'], ignore_index=True, inplace=True)   \n",
    "\n",
    "            words_df = self.combine_key_value_pairs_in_words_df(words_df)\n",
    "\n",
    "            split_lines     = list()\n",
    "            split_lines_row = list()\n",
    "            split_lines_col = list()\n",
    "            drop_idxs = list()\n",
    "            for rIdx, line in subsect_df.iterrows():\n",
    "                text_by_col = line['text'].split(self.col_sep_str)\n",
    "                split_lines.extend(text_by_col)\n",
    "                split_lines_row.extend([rIdx for k in range(len(text_by_col))])\n",
    "                split_lines_col.extend(list(range(len(text_by_col))))\n",
    "                \n",
    "                # combine words_df to match phrases in split_lines\n",
    "                for tIdx, token in enumerate(text_by_col):\n",
    "                    combine_idxs = [idx for idx, word in words_df.iterrows() \n",
    "                                    if  word['text'] in token\n",
    "                                    and word['top']+0.5 >= line['top']\n",
    "                                    and word['bottom'] <= line['bottom']+5]\n",
    "                    tmp_df = words_df.loc[combine_idxs, :]\n",
    "                    \n",
    "                    first_word = token.split(' ')[0]\n",
    "                    possible_start_idx = tmp_df.index[tmp_df['text'] == first_word]\n",
    "                    for start_idx in possible_start_idx:\n",
    "                        phrase_idxs = range(start_idx, start_idx+len(token.split(' ')))\n",
    "                        if all([True if idx in tmp_df.index else False for idx in phrase_idxs]): \n",
    "                            phrase = ' '.join(tmp_df.loc[phrase_idxs, 'text'])\n",
    "                            if phrase == token:\n",
    "                                words_df.loc[start_idx, ['text', 'right']] = [phrase, tmp_df.loc[phrase_idxs[-1], 'right']]\n",
    "                                drop_idxs.extend(phrase_idxs[1:])             \n",
    "                                break \n",
    "\n",
    "            words_df = words_df.drop(drop_idxs).reset_index(drop=True)\n",
    "\n",
    "            if not all([True if phrase==token else False for phrase, token in zip(split_lines, words_df['text'])]):\n",
    "                raise ValueError('the phrases in split_lines do not match the corrected tokens in words_df')\n",
    "\n",
    "            for token, rIdx, cIdx, (wIdx, word) in zip(split_lines, split_lines_row, split_lines_col, words_df.iterrows()):\n",
    "                if self.key_val_sep in token:\n",
    "                    continue\n",
    "                token_bounds = word[['left', 'right']]\n",
    "                col_mask = ((words_df['right' ]- token_bounds['left'] > -left_mult*self.px_col_sep) & \\\n",
    "                            (words_df['left' ] - token_bounds['left'] <=  0                )) | \\\n",
    "                           ((words_df['right'] - token_bounds['right'] < right_mult*self.px_col_sep) & \\\n",
    "                            (words_df['right'] - token_bounds['right'] >= 0                ))\n",
    "                same_column_tokens = words_df.loc[col_mask, 'text']\n",
    "                \n",
    "                implicit_key = [item.split(self.key_val_sep)[0] for item in same_column_tokens.values if len(item.split(self.key_val_sep)) == 2]\n",
    "                if len(implicit_key) >= 1:\n",
    "                    original_text = self.text_df.loc[rIdx, 'text']\n",
    "                    text_cols = original_text.split(self.col_sep_str)\n",
    "                    text_cols[cIdx] = f'{implicit_key[0]}{self.key_val_sep}{text_cols[cIdx]}'\n",
    "                    self.text_df.loc[rIdx, 'text'] = self.col_sep_str.join(text_cols)  \n",
    "\n",
    "    def identify_line_merge_sets(self, text_df=None):\n",
    "\n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "        \n",
    "        is_ocr = True if any(text_df['source'] == 'ocr') else False\n",
    "\n",
    "        merge_sets = list()\n",
    "        for idx, line in text_df.iterrows():\n",
    "            if 'using' in line.text:\n",
    "                stop = []\n",
    "            if is_ocr:\n",
    "                same_line_overlap_thresh = 0.0048\n",
    "                # # top of word begins above bottom of current word, by at least 0.005 (.5% of page), and starts below top of word  \n",
    "                # merge_set = np.where((text_df['norm_top']    < line['norm_bottom']) &\n",
    "                #                     (np.abs(text_df['norm_top'] - line['norm_bottom']) > same_line_overlap_thresh) &\n",
    "                #                     (text_df['norm_top']   >= line['norm_top']   )  )[0]\n",
    "                # words that are below current word and overlap by at least same_line_overlap_thresh, or above current word and overlap by at least same \n",
    "                merge_set = np.where(((text_df['norm_top'] - line['norm_bottom'] < -same_line_overlap_thresh) &\n",
    "                                      (text_df['norm_top'] >= line['norm_top'])                           \n",
    "                                      ) |\n",
    "                                     ((text_df['norm_bottom'] - line['norm_top'] >  same_line_overlap_thresh) & \n",
    "                                      (text_df['norm_top'] <= line['norm_top'])))\n",
    "            else:\n",
    "                merge_set = np.where((text_df['norm_top']    < line['norm_bottom']) &\n",
    "                                    (text_df['norm_top']    > line['norm_top']   )  )[0]\n",
    "                \n",
    "            merge_set = text_df.index[merge_set]\n",
    "            if len(merge_set) > 0:\n",
    "                merge_set = merge_set.to_list()\n",
    "                merge_set = sorted(merge_set + [idx]) if idx not in merge_set else merge_set \n",
    "                same_merge_set   = any([True if m_set == merge_set else False for m_set in merge_sets])\n",
    "                overlapping_sets = [set_idx for set_idx, m_set in enumerate(merge_sets) if any(i for i in m_set if i in merge_set)]\n",
    "                if same_merge_set:\n",
    "                    continue\n",
    "                elif len(overlapping_sets) == 1:\n",
    "                    merge_sets[overlapping_sets[0]] = np.unique(merge_sets[overlapping_sets[0]] + merge_set).tolist() \n",
    "                elif len(overlapping_sets) > 1:\n",
    "                    print('Have not written code to manage more than one overlapping set when combining lines')\n",
    "                else:\n",
    "                    merge_sets.append(merge_set)\n",
    "                    print(' '.join(text_df.loc[merge_set,['text','left']].sort_values(by='left').text))\n",
    "        \n",
    "        text_df['line_idx'] = np.full((text_df.shape[0],), -1, dtype=int)\n",
    "        for msIdx, merge_set in enumerate(merge_sets):\n",
    "            text_df.loc[merge_set, 'line_idx'] = np.repeat(msIdx, len(merge_set))\n",
    "\n",
    "        return merge_sets, text_df\n",
    "        \n",
    "    def get_words_df(self, page, top, bottom):\n",
    "        page_crop = page.within_bbox((         0, top, \n",
    "                                        page.width, bottom))  \n",
    "\n",
    "        words = page_crop.extract_words()  \n",
    "        words_dict = dict(text=[], left=[], right=[], top=[], bottom=[])\n",
    "        for word in words:\n",
    "            if word['text'] == '|':\n",
    "                continue\n",
    "            word['text'] = word['text'].lower().replace('|','')\n",
    "            for dict_key, word_key in zip(['text', 'left', 'right', 'top', 'bottom'],\n",
    "                                            ['text',   'x0',    'x1', 'top', 'bottom']):\n",
    "                words_dict[dict_key].append(word[word_key])\n",
    "        \n",
    "        words_df = pd.DataFrame.from_dict(words_dict)\n",
    "        words_df.sort_values(by='left', ignore_index=True, inplace=True)\n",
    "\n",
    "        return words_df\n",
    "\n",
    "    def identify_columns_from_words_df(self, words_df):\n",
    "        col_id = []\n",
    "        col_num = 0\n",
    "        prev_w_info = None\n",
    "        for w_idx, w_info in words_df.iterrows():\n",
    "            if prev_w_info is not None: \n",
    "                if (w_info['left'] - prev_w_info['right'] > self.px_col_sep):\n",
    "                    col_num += 1\n",
    "                elif (w_info['left'] - prev_w_info['right'] < 0): \n",
    "                    w_info['right'] = max([prev_w_info['right'], w_info['right']])\n",
    "                # FIXME may need to reinstate this condition for original text\n",
    "                # elif (w_info['left'] - prev_w_info['right'] < 0): \n",
    "                #     w_info['right'] = prev_w_info['right'] \n",
    "            col_id.append(col_num)\n",
    "            prev_w_info = w_info.copy()\n",
    "        words_df['col_id'] = col_id\n",
    "        if 'source' in words_df.columns and any(words_df['source'] == 'ocr'):\n",
    "            words_df['top'] = round(words_df['top'], -1)\n",
    "        words_df.sort_values(by=['col_id', 'top', 'left'], ignore_index=True, inplace=True)\n",
    "\n",
    "        col_phrases = []\n",
    "        for col_id in words_df['col_id'].unique():\n",
    "            col_df = words_df.loc[words_df['col_id'] == col_id, :]\n",
    "            col_phrases.append(' '.join(col_df['text']))\n",
    "\n",
    "        return col_phrases \n",
    "\n",
    "    def organize_single_lines(self, merge_sets, ocr_pdf, orig_pdf, text_df=None):\n",
    "        \n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "\n",
    "        merge_list = list(itertools.chain.from_iterable(merge_sets))\n",
    "        for idx, line in text_df.iterrows():\n",
    "            if idx not in merge_list:   \n",
    "                if line['source'] == 'ocr':\n",
    "                    page = ocr_pdf.pages [line['page']]\n",
    "                else:\n",
    "                    page = orig_pdf.pages[line['page']]\n",
    "\n",
    "                words_df = self.get_words_df(page, line['top'], line['bottom'])\n",
    "                words_df.loc[:, 'top'] = np.round(words_df['top']) \n",
    "\n",
    "                words_df = self.combine_key_value_pairs_in_words_df(words_df)\n",
    "\n",
    "                col_phrases = self.identify_columns_from_words_df(words_df)\n",
    "\n",
    "                text_df.loc[idx, 'text'] = self.col_sep_str.join(col_phrases)\n",
    "        return text_df\n",
    "\n",
    "    def condense_merge_sets(self, text_df = None, ocr_pdf = None, orig_pdf = None):\n",
    "\n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "        is_ocr = True if any(text_df['source'] == 'ocr') else False\n",
    "\n",
    "        drop_idxs = list()\n",
    "        for line_idx in text_df['line_idx'].unique():\n",
    "            if line_idx == -1:\n",
    "                continue\n",
    "            line_df = text_df.loc[text_df['line_idx'] == line_idx, :]\n",
    "            top     = line_df['top'].min()    \n",
    "            bottom  = line_df['bottom'].max()\n",
    "\n",
    "            if not is_ocr:\n",
    "                page = orig_pdf.pages[line_df.loc[line_df.index[0], 'page']]\n",
    "                words_df = self.get_words_df(page, top, bottom)\n",
    "                col_phrases = self.identify_columns_from_words_df(words_df)\n",
    "                # replace first line in merge set with merged text and position info, then store indices of \n",
    "                # remaining merge set to drop at end of combine method\n",
    "                text_df.loc[line_df.index[0], \n",
    "                            ['text','bottom','left','right', 'norm_bottom','top','norm_top',]\n",
    "                            ] = [self.col_sep_str.join(col_phrases),\n",
    "                                line_df['bottom'].max(),\n",
    "                                line_df['left'].min(),\n",
    "                                line_df['right'].max(),\n",
    "                                line_df['norm_bottom'].max(),\n",
    "                                line_df['top'].min(),\n",
    "                                line_df['norm_top'].min(),] \n",
    "\n",
    "            drop_idxs.extend(line_df.index[1:]) \n",
    "\n",
    "        text_df = text_df.drop(drop_idxs).reset_index(drop=True)\n",
    "        return text_df\n",
    "\n",
    "\n",
    "    def organize_text_lines_by_row_and_column(self, text_df=None):\n",
    "        merge_sets, text_df = self.identify_line_merge_sets(text_df)\n",
    "        orig_pdf = pdfplumber.open(self.orig_filepath) \n",
    "        text_df = self.organize_single_lines(merge_sets, None, orig_pdf, text_df)\n",
    "        text_df = self.condense_merge_sets   (text_df, None, orig_pdf)\n",
    "        return text_df\n",
    "    \n",
    "    def split_text_lines_with_pdfplumber(self, pdf, element, page_idx, text_dict,):\n",
    "        did_split = False\n",
    "        page = pdf.pages[page_idx]\n",
    "        page_crop = page.within_bbox((         0, page.height - element.y1, \n",
    "                                      page.width, page.height - element.y0))  \n",
    "\n",
    "        text = page_crop.extract_text_lines()\n",
    "        \n",
    "        if len(text) > 1:\n",
    "            did_split = True\n",
    "            for line in text:\n",
    "\n",
    "                norm_top    = page_idx + (line['top'   ] / page.height)\n",
    "                norm_bottom = page_idx + (line['bottom'] / page.height)\n",
    "                if np.logical_not(np.any((np.isclose(self.text_df['norm_top'   ], norm_top   , atol=1e-2)) &\n",
    "                                         (np.isclose(self.text_df['norm_bottom'], norm_bottom, atol=1e-2))  )):\n",
    "                    text_dict['page'       ].append(page_idx)\n",
    "                    text_dict['bottom'     ].append(line['bottom'])\n",
    "                    text_dict['top'        ].append(line['top'])\n",
    "                    text_dict['left'       ].append(line['x0'])\n",
    "                    text_dict['right'      ].append(line['x1'])\n",
    "                    text_dict['text'       ].append(line['text'].lower().replace('|', ''))\n",
    "                    text_dict['norm_top'   ].append(norm_top)\n",
    "                    text_dict['norm_bottom'].append(norm_bottom)\n",
    "                    text_dict['source'     ].append('ocr')\n",
    "                    \n",
    "                    print(f'\\nstoring text from OCR on page {page_idx}:\\n\"{line['text'].lower()}\"')\n",
    "\n",
    "        return did_split    \n",
    "\n",
    "    def get_text_lines_from_original(self):\n",
    "        pdf = pdfplumber.open(self.orig_filepath)\n",
    "        text_dict = dict(text=[], source=[], page=[], top=[], bottom=[], left=[], right=[], norm_top=[], norm_bottom=[], confidence=[])\n",
    "        for page_idx, page in enumerate(pdf.pages):\n",
    "            text_lines = page.extract_text_lines()\n",
    "            for line in text_lines:\n",
    "                norm_top    = page_idx + (line['top'   ] / page.height)\n",
    "                norm_bottom = page_idx + (line['bottom'] / page.height)\n",
    "\n",
    "                text_dict['page'       ].append(page_idx)\n",
    "                text_dict['bottom'     ].append(line['bottom'])\n",
    "                text_dict['top'        ].append(line['top'])\n",
    "                text_dict['left'       ].append(line['x0'])\n",
    "                text_dict['right'      ].append(line['x1'])\n",
    "                text_dict['text'       ].append(line['text'].lower().replace('|', ''))\n",
    "                text_dict['norm_top'   ].append(norm_top)\n",
    "                text_dict['norm_bottom'].append(norm_bottom)\n",
    "                text_dict['source'     ].append('original')\n",
    "                text_dict['confidence' ].append(1.0)\n",
    "        \n",
    "        text_df = pd.DataFrame(text_dict)\n",
    "        text_df = self.organize_text_lines_by_row_and_column(text_df=text_df)\n",
    "\n",
    "        if self.text_df is None:\n",
    "            self.text_df = text_df\n",
    "        elif len(text_df) > 0:\n",
    "            self.add_new_data_to_text_df(text_df)\n",
    "\n",
    "        self.text_df.sort_values(by='norm_top', inplace=True, ignore_index=True, ascending=True)\n",
    "\n",
    "    def identify_columns_from_words_df_docTR(self, words_df):\n",
    "        col_id = []\n",
    "        col_num = 0\n",
    "        prev_w_info = None\n",
    "        for w_idx, w_info in words_df.iterrows():\n",
    "            if prev_w_info is not None: \n",
    "                if (w_info['left'] - prev_w_info['right'] > self.px_col_sep):\n",
    "                    col_num += 1\n",
    "                elif (w_info['left'] - prev_w_info['right'] < 0):\n",
    "                    w_info['right'] = prev_w_info['right'] \n",
    "            col_id.append(col_num)\n",
    "            prev_w_info = w_info.copy()\n",
    "        words_df['col_id'] = col_id\n",
    "        words_df.sort_values(by=['col_id', 'top', 'left'], ignore_index=True, inplace=True)\n",
    "\n",
    "        col_phrases = []\n",
    "        for col_id in words_df['col_id'].unique():\n",
    "            col_df = words_df.loc[words_df['col_id'] == col_id, :]\n",
    "            col_phrases.append(' '.join(col_df['text']))\n",
    "\n",
    "        return col_phrases \n",
    "\n",
    "    def organize_single_lines_docTR(self, text_df=None):\n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "        \n",
    "        drop_idxs = list()\n",
    "        for line_idx in text_df['line_idx'].unique():\n",
    "            line_df = text_df.loc[text_df['line_idx'] == line_idx, :]\n",
    "\n",
    "            line_df.index.names=['original_index']\n",
    "            line_df = line_df.sort_values(by='left', \n",
    "                                          ascending=True, \n",
    "                                          ignore_index=False, \n",
    "                                          inplace=False\n",
    "                                          )\n",
    "            line_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "            drop_idxs.extend(line_df['original_index'].values[1:].tolist())\n",
    "\n",
    "            if line_idx == 2:\n",
    "                stop  = [] \n",
    "            line_df = self.combine_key_value_pairs_in_words_df(line_df)\n",
    "\n",
    "            col_phrases = self.identify_columns_from_words_df(line_df.copy())\n",
    "\n",
    "            text_df.loc[line_df.loc[0, 'original_index'], 'text'] = self.col_sep_str.join(col_phrases)\n",
    "\n",
    "        self.ocr_words_df = text_df.copy()\n",
    "        text_df = text_df.drop(drop_idxs).reset_index(drop=True)\n",
    "\n",
    "        return text_df\n",
    "\n",
    "    def combine_merge_sets_docTR(self, text_df=None):\n",
    "\n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "\n",
    "        drop_idxs = list()\n",
    "        for line_idx in text_df['line_idx'].unique():\n",
    "            line_df = text_df.loc[text_df['line_idx'] == line_idx, :]\n",
    "\n",
    "            # replace first line in merge set with merged text and position info, then store indices of \n",
    "            # remaining merge set to drop at end of combine method\n",
    "            text_df.loc[line_df.index[0], \n",
    "                        ['bottom', \n",
    "                         'left', \n",
    "                         'right', \n",
    "                         'norm_bottom',\n",
    "                         'top',\n",
    "                         'norm_top',\n",
    "                         ]] = [line_df['bottom'].max(),\n",
    "                               line_df['left'].min(),\n",
    "                               line_df['right'].max(),\n",
    "                               line_df['norm_bottom'].max(),\n",
    "                               line_df['top'].min(),\n",
    "                               line_df['norm_top'].min(),] \n",
    "\n",
    "            drop_idxs.extend(line_df.index[1:])             \n",
    "\n",
    "        text_df = text_df.drop(drop_idxs).reset_index(drop=True)\n",
    "\n",
    "    def identify_line_merge_sets_docTR(self, text_df=None):\n",
    "        \n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "        \n",
    "        same_line_overlap_thresh = 0.005\n",
    "\n",
    "        merge_sets = list()\n",
    "        for idx, line in text_df.iterrows():\n",
    "            merge_set = np.where((text_df['norm_top']    < line['norm_bottom']) &\n",
    "                                 (np.abs(text_df['norm_top'] - line['norm_bottom']) > same_line_overlap_thresh) &\n",
    "                                 (text_df['norm_top']   >= line['norm_top']   )  )[0]\n",
    "            merge_set = text_df.index[merge_set]\n",
    "            if len(merge_set) > 0:\n",
    "                merge_set = merge_set.to_list()\n",
    "                merge_set = sorted(merge_set.append(idx)) if idx not in merge_set else merge_set \n",
    "                same_merge_set   = any([True if m_set == merge_set else False for m_set in merge_sets])\n",
    "                overlapping_sets = [set_idx for set_idx, m_set in enumerate(merge_sets) if any(i for i in m_set if i in merge_set)]\n",
    "                if same_merge_set:\n",
    "                    continue\n",
    "                elif len(overlapping_sets) == 1:\n",
    "                    merge_sets[overlapping_sets[0]] = np.unique(merge_sets[overlapping_sets[0]] + merge_set).tolist() \n",
    "                elif len(overlapping_sets) > 1:\n",
    "                    print('Have not written code to manage more than one overlapping set when combining lines')\n",
    "                else:\n",
    "                    merge_sets.append(merge_set)\n",
    "                    print(' '.join(text_df.loc[merge_set,['text','left']].sort_values(by='left').text))\n",
    "        text_df['line_idx'] = np.full((text_df.shape[0],), -1, dtype=int)\n",
    "        for msIdx, merge_set in enumerate(merge_sets):\n",
    "            text_df.loc[merge_set, 'line_idx'] = np.repeat(msIdx, len(merge_set))\n",
    "\n",
    "        return text_df\n",
    "\n",
    "    def organize_text_lines_by_row_and_column_docTR(self, text_df=None):\n",
    "        # text_df = self.identify_line_merge_sets_docTR(text_df)\n",
    "        merge_sets, text_df = self.identify_line_merge_sets(text_df)\n",
    "        text_df = self.organize_single_lines_docTR(text_df) #TODO 1 Figure out why frequency is dropped to next line in \"using unlicensed frequencies\", and correct incorrect word ordering (see photo on phone)\n",
    "        # self.ocr_words_df = text_df.copy()\n",
    "        # text_df = self.condense_merge_sets(text_df, None, None)\n",
    "        return text_df\n",
    "\n",
    "    def add_new_data_to_text_df(self, tmp_text_df):\n",
    "        drop_idxs =list()\n",
    "        for idx, line in tmp_text_df.iterrows():\n",
    "            match_score = np.array([levenshtein.normalized_similarity(line['text'], match_line['text']) for rIdx, match_line in self.text_df.iterrows()])\n",
    "            if any(match_score > 0.8):\n",
    "                drop_idxs.append(idx)\n",
    "\n",
    "        tmp_text_df = tmp_text_df.drop(drop_idxs)\n",
    "\n",
    "        self.text_df = pd.concat((self.text_df, tmp_text_df), axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    def get_text_lines_from_ocr(self, pages: Optional[Iterable[int]] = None) -> pd.DataFrame:\n",
    "        text_dict = dict(text=[], source=[], page=[], top=[], bottom=[], left=[], right=[], norm_top=[], norm_bottom=[], confidence=[])\n",
    "        for page_idx, page in enumerate(self.ocr_text['pages']):\n",
    "            for block in page['blocks']:\n",
    "                for line in block['lines']:\n",
    "                    for word in line['words']:\n",
    "                        text_dict['page'       ].append(page_idx)\n",
    "                        text_dict['bottom'     ].append(page['dimensions'][0] * word['geometry'][1][1])\n",
    "                        text_dict['top'        ].append(page['dimensions'][0] * word['geometry'][0][1])\n",
    "                        text_dict['left'       ].append(page['dimensions'][1] * word['geometry'][0][0])\n",
    "                        text_dict['right'      ].append(page['dimensions'][1] * word['geometry'][1][0])\n",
    "                        text_dict['text'       ].append(word['value'].lower())\n",
    "                        text_dict['norm_top'   ].append(page_idx+word['geometry'][0][1])\n",
    "                        text_dict['norm_bottom'].append(page_idx+word['geometry'][1][1])\n",
    "                        text_dict['confidence' ].append(word['confidence'])\n",
    "                        text_dict['source'     ].append('ocr') \n",
    "        ocr_text_df = pd.DataFrame(text_dict)\n",
    "        ocr_text_df = ocr_text_df.sort_values(by=['norm_top', 'left'], ascending=True, ignore_index=True)\n",
    "\n",
    "        ocr_text_df = self.organize_text_lines_by_row_and_column_docTR(text_df=ocr_text_df)        \n",
    "        \n",
    "        if self.text_df is None:\n",
    "            self.text_df = ocr_text_df\n",
    "        elif len(ocr_text_df) > 0:\n",
    "            self.add_new_data_to_text_df(ocr_text_df)\n",
    "\n",
    "        self.text_df.sort_values(by='norm_top', inplace=True, ignore_index=True, ascending=True)\n",
    "\n",
    "    def get_text_lines_from_ocr_AFTER_TESSERACT(self, pages: Optional[Iterable[int]] = None) -> pd.DataFrame:\n",
    "        text_dict = dict(text=[], source=[], page=[], top=[], bottom=[], left=[], right=[], norm_top=[], norm_bottom=[],)\n",
    "        for page_idx, page in enumerate(extract_pages(self.ocr_filepath)):\n",
    "            pdf = pdfplumber.open(self.ocr_filepath)\n",
    "            for el_idx, element in enumerate(page):\n",
    "                if isinstance(element, LTTextContainer):\n",
    "                    did_split = self.split_text_lines_with_pdfplumber(pdf, element, page_idx, text_dict)\n",
    "                    if not did_split:\n",
    "                        top = page.height - element.y1\n",
    "                        bottom = page.height - element.y0 \n",
    "                        norm_top    = page_idx + top    / page.height\n",
    "                        norm_bottom = page_idx + bottom / page.height\n",
    "                        if np.logical_not(np.any((np.isclose(self.text_df['norm_top'   ], norm_top   , atol=1e-2)) &\n",
    "                                                 (np.isclose(self.text_df['norm_bottom'], norm_bottom, atol=1e-2))  )):\n",
    "                            text_dict['page'       ].append(page_idx)\n",
    "                            text_dict['bottom'     ].append(bottom)\n",
    "                            text_dict['top'        ].append(top)\n",
    "                            text_dict['left'       ].append(element.x0)\n",
    "                            text_dict['right'      ].append(element.x1)\n",
    "                            text_dict['text'       ].append(element.get_text().lower().replace('\\n', '').replace('|', ''))\n",
    "                            text_dict['norm_top'   ].append(norm_top)\n",
    "                            text_dict['norm_bottom'].append(norm_bottom)\n",
    "                            text_dict['source'     ].append('ocr')\n",
    "    \n",
    "                            print(f'\\nstoring text from OCR on page {page_idx}:\\n\"{element.get_text().lower().replace('\\n', '')}\"')\n",
    "\n",
    "                elif isinstance(element, LTRect):\n",
    "                    print(f'page = {page_idx}, ypos={element.y0}, type={type(element)}')\n",
    "                else:\n",
    "                    print(f'page = {page_idx}, ypos={element.y0}, type={type(element)}')\n",
    "\n",
    "        if self.text_df is None:\n",
    "            self.text_df = pd.DataFrame(text_dict)\n",
    "        elif len(text_dict['text']) > 0:\n",
    "            tmp_text_df  = pd.DataFrame(text_dict)\n",
    "            self.text_df = pd.concat((self.text_df, tmp_text_df), axis=0, ignore_index=True)\n",
    "\n",
    "        self.text_df.sort_values(by='norm_top', inplace=True, ignore_index=True, ascending=True)\n",
    "\n",
    "    def get_section_headers(self):\n",
    "        with open(self.config, 'r') as file:\n",
    "            self.config_data = yaml.safe_load(file) \n",
    "        \n",
    "        self.sections = dict()\n",
    "        for section in self.config_data['sections']:\n",
    "            self.sections[section['header']] = dict(bounds = pd.DataFrame(),\n",
    "                                                    extract = section['extract'])\n",
    "            for key in section.keys():\n",
    "                if key in ['header', 'extract']: continue\n",
    "                self.sections[section['header']][key] = section[key]\n",
    "\n",
    "    def get_section_bounds(self):\n",
    "\n",
    "        self.get_section_headers()\n",
    "\n",
    "        # detect start and end of sections\n",
    "        prev_section_info = dict(header=None, df_row=None)\n",
    "        prev_page = 0\n",
    "        for row_idx, text_data in self.text_df.iterrows():\n",
    "            detected_section = None\n",
    "            for header in self.sections.keys():\n",
    "                match = regex.search(f'{header}{{s<=3,i<=3,d<=3}}', text_data['text'], regex.BESTMATCH)    \n",
    "                if match is not None:\n",
    "                    detected_section = header\n",
    "                    break\n",
    "\n",
    "            if text_data['page'] > prev_page or detected_section is not None:\n",
    "                if prev_section_info['header'] is not None:   # store end of sections\n",
    "                    prev_df_idx = self.sections[prev_section_info['header']]['bounds'].index[-1] \n",
    "                    prev_bottom = prev_page+0.9999 if (text_data['page'] > prev_page) else text_data['norm_top'] \n",
    "                    self.sections[prev_section_info['header']]['bounds'].loc[prev_df_idx, ['bottom',]] = [prev_bottom]\n",
    "                # store start of sections\n",
    "                top = text_data['page'] if (text_data['page'] > prev_page) else text_data['norm_top']\n",
    "                tmp_df = pd.DataFrame(data    = zip([top], [np.nan], [False]),\n",
    "                                      columns = ['top', 'bottom', 'data_extracted'],)\n",
    "                header = detected_section\n",
    "                self.sections[header]['bounds'] = pd.concat((self.sections[header]['bounds'], tmp_df), ignore_index=True)\n",
    "                prev_section_info['header'] = header\n",
    "                if text_data['page'] > prev_page:\n",
    "                    prev_page+=1\n",
    "            prev_section_info['df_row'] = row_idx\n",
    "        \n",
    "        # store end of final section\n",
    "        prev_bottom = self.text_df.loc[prev_section_info['df_row'], 'norm_bottom']\n",
    "        prev_df_idx = self.sections[prev_section_info['header']]['bounds'].index[-1] \n",
    "        self.sections[prev_section_info['header']]['bounds'].loc[prev_df_idx, ['bottom',]] = [np.ceil(prev_bottom),]\n",
    "    \n",
    "    def extract_table(self, subsection_bounds):  \n",
    "        page_num = int(np.floor(subsection_bounds['top'])) \n",
    "        pdf = pdfplumber.open(self.orig_filepath)\n",
    "        table_page = pdf.pages[page_num]\n",
    "        table_crop = table_page.within_bbox((               0, (subsection_bounds['top'   ]-page_num)*table_page.height, \n",
    "                                             table_page.width, (subsection_bounds['bottom']-page_num)*table_page.height))\n",
    "        table = table_crop.extract_table()\n",
    "        # print((subsection_bounds['top'   ]-page_num)*table_page.height, (subsection_bounds['bottom']-page_num)*table_page.height, table)\n",
    "        return table\n",
    "    \n",
    "    def extract_text_lines_from_original(self, subsection_bounds):\n",
    "        page_num = int(np.floor(subsection_bounds['top'])) \n",
    "        pdf = pdfplumber.open(self.orig_filepath)\n",
    "        page = pdf.pages[page_num]\n",
    "        page_crop = page.within_bbox((         0, (subsection_bounds['top'   ]-page_num)*page.height, \n",
    "                                      page.width, (subsection_bounds['bottom']-page_num)*page.height))  \n",
    "\n",
    "        text = page_crop.extract_text_lines()\n",
    "\n",
    "        for line in text:\n",
    "            line['norm_top']    = page_num + (line['top']    / page.height)\n",
    "            line['norm_bottom'] = page_num + (line['bottom'] / page.height)\n",
    "            line['page']        = page_num\n",
    "            line['bottom']      = page.height - line['bottom']\n",
    "            line['top']         = page.height - line['top']\n",
    "            line['text']        = line['text'].lower().replace('|', '')\n",
    "\n",
    "        return text  \n",
    "\n",
    "    def table_to_df(self, \n",
    "                    table: List[List[str]], \n",
    "                    extract_params: str | Dict | List[Dict],\n",
    "                    ) -> pd.DataFrame:\n",
    "        if type(extract_params) == str and 'col' in extract_params.lower():\n",
    "            info_keys = list()\n",
    "            data = list()\n",
    "            for row in table:\n",
    "                iKey = row.pop(0)\n",
    "                info_keys.append(iKey.replace('\\n', ' '))\n",
    "                data.append(row)\n",
    "            df = pd.DataFrame(data=np.array(data).transpose(), columns=info_keys)\n",
    "\n",
    "        elif type(extract_params) == str and 'row' in extract_params.lower():\n",
    "            info_keys = table[0]\n",
    "            data = table[1:]\n",
    "            df = pd.DataFrame(data=data, columns=info_keys)\n",
    "\n",
    "        else:\n",
    "            print('There is no method implemented for converting data with this extraction method to a DataFrame')\n",
    "            df = None\n",
    "        \n",
    "        return df \n",
    " \n",
    "    def extract_table_data(self):\n",
    "        for section_header, section_dict in self.sections.items():\n",
    "            for sub_idx, subsection_bounds in section_dict['bounds'].iterrows():\n",
    "                if subsection_bounds['data_extracted']:\n",
    "                    continue\n",
    "\n",
    "                table = self.extract_table(subsection_bounds)\n",
    "                if table is not None:\n",
    "                    data_df = self.table_to_df(table, section_dict['extract'])\n",
    "                    data_df = data_df.loc[:, [col for col in data_df.columns if col.lower() != section_header]]\n",
    "                    if 'data' in section_dict.keys(): \n",
    "                        section_dict['data'] = pd.concat((section_dict['data'], data_df),\n",
    "                                                         axis=0,\n",
    "                                                         ignore_index=True)\n",
    "                    else:\n",
    "                        section_dict['data'] = data_df\n",
    "                    \n",
    "                    section_dict['bounds'].loc[sub_idx, 'data_extracted'] = True\n",
    "\n",
    "    def align_data_to_existing_df(self,\n",
    "                                  section_dict: Dict,\n",
    "                                  subsect_df: pd.DataFrame,\n",
    "                                 ):\n",
    "        if type(section_dict['extract']) == str and 'col' in section_dict['extract'].lower():\n",
    "            info_keys_to_match = list(section_dict['data'].columns)\n",
    "            original_info_keys = list(section_dict['data'].columns)\n",
    "            info_keys = list()\n",
    "            data      = list()\n",
    "            for text in subsect_df['text']:\n",
    "                row = text.split(self.col_sep_str)\n",
    "                iKey = row.pop(0)\n",
    "                iKey = iKey.replace('\\n', ' ')\n",
    "                match_score = np.array([levenshtein.normalized_similarity(iKey.lower(), matchKey.lower()) for matchKey in info_keys_to_match])\n",
    "                sorted_match_score, sorted_info_keys = zip(*sorted(zip(match_score, info_keys_to_match), reverse=True))\n",
    "                if iKey == 'equipment tip':\n",
    "                    stop = []\n",
    "                try:\n",
    "                    top_key_matches = sorted_info_keys[:3]\n",
    "                    \n",
    "                    #correct any instances in which OCR dropped the last word (probably b/c it was on a second line), causing poor matching\n",
    "                    if (sorted_match_score[0] < 0.9 \n",
    "                        and len(iKey.split(' ')) == len(top_key_matches[0].split(' ')) \n",
    "                        and any([len(iKey.split(' ')) < len(key.split(' ')) for key in top_key_matches])):\n",
    "                        match_score = np.array([levenshtein.normalized_similarity(iKey.lower(), ' '.join(matchKey.lower().split(' ')[:-1])) for matchKey in info_keys_to_match])\n",
    "            \n",
    "                    matched_key = info_keys_to_match.pop(np.where(match_score == match_score.max())[0][0])\n",
    "                    info_keys.append(matched_key)\n",
    "                    data.append(row)\n",
    "                    print(iKey, matched_key)\n",
    "                except:\n",
    "                    print(f'\\nNo matched key: iKey={iKey}, text={text}')\n",
    "            \n",
    "            # Add dummy data for unmatched keys\n",
    "            expected_num_items = pd.Series([len(d) for d in data]).mode()[0]\n",
    "            for key in info_keys_to_match:\n",
    "                info_keys.append(key)\n",
    "                data.append(['data_not_found' for k in range(expected_num_items)])\n",
    "\n",
    "            # expected_num_items = pd.Series([len(d) for d in data]).mode()[0]\n",
    "            wrong_count_info = [(idx, len(d)) for idx, d in enumerate(data) if len(d) != expected_num_items]\n",
    "            for idx, num_items in wrong_count_info:\n",
    "                data[idx] = ['wrong_num_columns' for k in range(expected_num_items)]\n",
    "                print(f'\\n\"{info_keys[idx]}\" contained the wrong number of columns in the line.')\n",
    "\n",
    "            correct_order = [np.where(np.array(original_info_keys) == key)[0][0] for key in info_keys]\n",
    "            _, info_keys = zip(*sorted(zip(correct_order, info_keys)))\n",
    "            _, data      = zip(*sorted(zip(correct_order, data))) \n",
    "\n",
    "            data_df = pd.DataFrame(data=np.array(data).transpose(), columns=info_keys)\n",
    "            section_dict['data'] = pd.concat((section_dict['data'], data_df),\n",
    "                                              axis=0,\n",
    "                                              ignore_index=True)\n",
    "        return\n",
    "\n",
    "    def get_multilevel_key_value_pairs(self, section_dict, items):\n",
    "        filling_subheader = False\n",
    "        for item in items: \n",
    "            split_item = item.split(self.key_val_sep)\n",
    "            if len(split_item) == 2:\n",
    "                key, value = split_item\n",
    "                if len(value) == 0:\n",
    "                    value = None\n",
    "                else:\n",
    "                    value = value[1:]  if value[0]  == ' '  else value\n",
    "                    value = value[:-1] if value[-1] == '\\n' else value\n",
    "                    \n",
    "                if filling_subheader:\n",
    "                    section_dict['data'][stored_key][key] = value\n",
    "                    print(f'{stored_key} - {key}{self.key_val_sep} {value}')\n",
    "                else:\n",
    "                    section_dict['data'][key] = value     \n",
    "                    print(f'{key}{self.key_val_sep} {value}')\n",
    "            elif len(split_item) == 1:\n",
    "                stored_key = split_item[0]\n",
    "                section_dict['data'][stored_key] = dict()\n",
    "                filling_subheader = True\n",
    "\n",
    "    def extract_key_value_pairs(self, \n",
    "                                section_dict: Dict, \n",
    "                                subsect_df: pd.DataFrame, \n",
    "                                ) -> Dict:\n",
    "        section_dict['data'] = dict()\n",
    "        for l_idx, line in subsect_df.iterrows(): \n",
    "            items = line['text'].split(self.col_sep_str)\n",
    "            key_value_pairs = [item for item in items if len(item.split(self.key_val_sep)) == 2]\n",
    "            if len(key_value_pairs) == len(items):\n",
    "                for item in key_value_pairs:\n",
    "                    key, value = item.split(self.key_val_sep)\n",
    "                    if len(value) == 0:\n",
    "                        value = None\n",
    "                    else:\n",
    "                        value = value[1:]  if value[0]  == ' '  else value\n",
    "                        value = value[:-1] if value[-1] == '\\n' else value\n",
    "                    section_dict['data'][key] = value     \n",
    "                    print(f'{key}{self.key_val_sep} {value}') \n",
    "            else:\n",
    "                if line['source'] == 'original':\n",
    "                    pdf = pdfplumber.open(self.orig_filepath)\n",
    "                elif line['source'] == 'ocr':\n",
    "                    pdf = pdfplumber.open(self.ocr_filepath)\n",
    "                self.get_multilevel_key_value_pairs(section_dict, items)\n",
    "\n",
    "    def extract_text_data(self):\n",
    "        for section_header, section_dict in self.sections.items():\n",
    "            for sub_idx, subsection_bounds in section_dict['bounds'].iterrows():\n",
    "                if subsection_bounds['data_extracted']:\n",
    "                    continue\n",
    "\n",
    "                mask = (self.text_df['norm_top'   ] > subsection_bounds['top'   ]) & \\\n",
    "                       (self.text_df['norm_bottom'] < subsection_bounds['bottom']) \n",
    "                subsect_df = self.text_df.loc[mask, :]\n",
    "                \n",
    "                if 'data' in section_dict.keys() and type(section_dict['data']) == pd.DataFrame:\n",
    "                    self.align_data_to_existing_df(section_dict, subsect_df)\n",
    "                else:\n",
    "                    self.extract_key_value_pairs(section_dict, subsect_df)\n",
    "                \n",
    "                section_dict['bounds'].loc[sub_idx, 'data_extracted'] = True\n",
    "\n",
    "    def print_text(self):\n",
    "        max_x = 0\n",
    "        for page_text in self.text_containers:\n",
    "            for text_container in page_text:\n",
    "                if text_container.x1 > max_x:\n",
    "                    max_x = text_container.x1\n",
    "        for page_num, page_text in enumerate(self.text_containers):\n",
    "            print('###############################################')\n",
    "            print(f'Page {page_num}')\n",
    "            print('###############################################') \n",
    "            for text_container in page_text:  \n",
    "                text = text_container.get_text()\n",
    "                # print(f'{text_container.y0} to {text_container.y1}', text)\n",
    "                print(text_container.y1, text_container.y0, text)\n",
    "\n",
    "    def convert_pdf_page_to_image(self, fitz_doc, image_path, idx, zoom=4):\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        page = fitz_doc.load_page(idx)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        pix.save(image_path)\n",
    "\n",
    "    def write_ocr_text_to_pdfa(self, pdf_outpath, hocr_path, image_path):\n",
    "        hocr = HocrTransform(hocr_filename=hocr_path,\n",
    "                            dpi=1000,)\n",
    "        hocr.to_pdf(\n",
    "                    out_filename=pdf_outpath,\n",
    "                    image_filename=image_path,\n",
    "                    )\n",
    "        \n",
    "    def write_hocr_xml_file(self, hocr_path, page_xml):\n",
    "        with open(hocr_path, 'w') as f:\n",
    "            f.write(page_xml[0].decode())\n",
    "\n",
    "    def create_pdfa_with_ocr(self, output_dir, output_stem, ocr_xml, fitz_doc):\n",
    "        merger = PdfMerger()\n",
    "        with TemporaryDirectory(dir= Path(os.getcwd())) as tmpdir:\n",
    "            tmppath = Path(tmpdir)\n",
    "            for idx, page_xml in enumerate(ocr_xml): \n",
    "                hocr_path   = tmppath / f'{output_stem}_hocr_page{idx}.xml'\n",
    "                image_path  = tmppath / f'{output_stem}_image_page{idx}.png'\n",
    "                pdf_outpath = tmppath / f'{output_stem}_docTR_page{idx}.pdf'\n",
    "                self.write_hocr_xml_file(hocr_path, page_xml)\n",
    "                self.convert_pdf_page_to_image(fitz_doc, image_path, idx, zoom=4) \n",
    "                self.write_ocr_text_to_pdfa(pdf_outpath, hocr_path, image_path)\n",
    "                merger.append(pdf_outpath)\n",
    "            merger.write(output_dir / f'{output_stem}_docTR.pdf' )\n",
    "            merger.close()\n",
    "\n",
    "    def run_ocr(self, ocr_predictor):\n",
    "        output_base_path = self.orig_filepath.parent / self.orig_filepath.stem\n",
    "        doc_pages = DocumentFile.from_pdf(self.orig_filepath)\n",
    "        fitz_doc = fitz.open(self.orig_filepath)\n",
    "        ocr_text = ocr_predictor(doc_pages) \n",
    "        self.ocr_text = ocr_text.export()\n",
    "        ocr_xml = ocr_text.export_as_xml()\n",
    "        self.create_pdfa_with_ocr(output_dir  = self.orig_filepath.parent, \n",
    "                                  output_stem = self.orig_filepath.stem, \n",
    "                                  ocr_xml = ocr_xml, \n",
    "                                  fitz_doc = fitz_doc)   \n",
    "\n",
    "\n",
    "class ATC_amendment(pdf_data):\n",
    "    def __init__(self, \n",
    "                 orig_filepath: str | Path, \n",
    "                 ocr_filepath:  str | Path, \n",
    "                 config:        str | Path,\n",
    "                 key_val_sep:   str = ':' ,) -> None:        \n",
    "        super().__init__(orig_filepath, ocr_filepath, config, key_val_sep)\n",
    "\n",
    "    def extract_line_config_data_from_cells_containing_all_key_value_pairs(self, \n",
    "                                                                           equipment_df, \n",
    "                                                                           configuration_col,\n",
    "                                                                           data_dict, \n",
    "                                                                           data_keys,\n",
    "                                                                           storage_key):\n",
    "        line_config_df = equipment_df.loc[:, configuration_col]\n",
    "        for equipIdx, cell in line_config_df.items():\n",
    "            for dKey in data_keys:\n",
    "                num_match = len(regex.findall(f'{dKey}{{e<=1}}', cell))\n",
    "                \n",
    "                if num_match > 0:\n",
    "                    # move thru each match (there may be multiple line configs in single cell)\n",
    "                    prev_search_end = 0\n",
    "                    for idx in range(num_match):\n",
    "                        match = regex.search(f'{dKey}{{e<=1}}', cell[prev_search_end:], pos=idx)\n",
    "                        key_span = (match.span()[0] + prev_search_end, match.span()[1] + prev_search_end)\n",
    "                        \n",
    "                        # find the next key match in the string to know where the value for this key ends\n",
    "                        possible_next_keys_pos = list()\n",
    "                        for next_dKey in data_keys:\n",
    "                            next_key_match = regex.search(f'{next_dKey}{{e<=1}}', cell[key_span[1]:], pos=0) \n",
    "                            if next_key_match is not None:\n",
    "                                possible_next_keys_pos.append(next_key_match.span()[0])\n",
    "                        # extract the data token from the string\n",
    "                        if len(possible_next_keys_pos) == 0:\n",
    "                            data_token = cell[key_span[0] : ]\n",
    "                        else:\n",
    "                            token_end = key_span[1] + min(possible_next_keys_pos) \n",
    "                            data_token = cell[key_span[0] : token_end]\n",
    "                            prev_search_end = token_end\n",
    "                        data_token = data_token[ :-1] if data_token[-1] == '\\n' else data_token\n",
    "                        data_token = data_token.replace('\\n', '')\n",
    "                        print(data_token)\n",
    "                        val = data_token.split(self.key_val_sep)[1]\n",
    "                        val = val[1:] if val[0] == ' ' else val\n",
    "\n",
    "                        data_dict[str(idx)][dKey][equipIdx] = val\n",
    "\n",
    "        for line_num, line_data in data_dict.items():\n",
    "            if any([True if any([True if val is not None else False for val in data_list]) else False \n",
    "                    for tmp_key, data_list in line_data.items()]):\n",
    "                for dKey, values in line_data.items():\n",
    "                    key = f'{storage_key}_{line_num}_{dKey}'\n",
    "                    equipment_df[key] = values \n",
    "        \n",
    "    def extract_line_config_data_from_separated_cells(self, \n",
    "                                                      equipment_df, \n",
    "                                                      configuration_cols,\n",
    "                                                      data_dict, \n",
    "                                                      data_keys,\n",
    "                                                      storage_key): \n",
    "        line_config_df = equipment_df.loc[:, configuration_cols]\n",
    "        for equipIdx, row in line_config_df.iterrows():\n",
    "            if levenshtein.normalized_similarity(row['Line Configuration'].lower(), 'n/a') < 0.3:\n",
    "                line_config = regex.split(r'\\n|;| - |-', row['Line Configuration']) # TODO 1 collect data by splitting on ; and -, store in data_dict (see photo on phone)\n",
    "                line_config  = [token.strip() for token in line_config if token != '']\n",
    "\n",
    "                # FIXME May need to fix this fix if it doesn't apply to other versions of old exhibit. \n",
    "                combine_idxs = [(idx, idx+1) \n",
    "                                for idx, (token, next_token) \n",
    "                                in enumerate(zip(line_config[:-1], line_config[1:])) \n",
    "                                if token[-1]=='\"' and next_token[0]=='(']  \n",
    "                for cIdxs in combine_idxs:\n",
    "                    line_config[cIdxs[0]] = ' '.join(line_config[cIdxs[0]:cIdxs[1]+1])\n",
    "                    del line_config[cIdxs[1]]\n",
    "                # fix end \n",
    "                \n",
    "                nKeys = len(data_dict['0'].keys())\n",
    "                if not len(line_config) % nKeys == 0:\n",
    "                    raise ValueError(\"The number of tokens in line_config is not a multiple of the expected number of keys for storing data\")\n",
    "                \n",
    "                equip_idx_list = [int(idx/nKeys) for idx in range(len(line_config))]\n",
    "                for dKey, token, equipIdx in zip(data_dict['0'].keys(), line_config, equip_idx_list):\n",
    "                    data_dict['0'][dKey][equipIdx] = token    \n",
    "\n",
    "            else: # TODO 2 deal with conduit vs line, make sure all data is stored appropriately\n",
    "                for dKey, col in zip(data_dict['0'].keys(), line_config_df.columns):\n",
    "                    data_dict['0'][dKey][equipIdx] = row[col]\n",
    "\n",
    "        for line_num, line_data in data_dict.items():\n",
    "            if levenshtein.normalized_similarity(line_data['Type'].lower(), 'conduit') > 0.5:\n",
    "                tmp_storage_key = 'conduit'\n",
    "            else:\n",
    "                tmp_storage_key = storage_key \n",
    "            if any([True if any([True if val is not None else False for val in data_list]) else False \n",
    "                    for tmp_key, data_list in line_data.items()]):\n",
    "                for dKey, values in line_data.items():\n",
    "                    key = f'{tmp_storage_key}_{line_num}_{dKey}'\n",
    "                    equipment_df[key] = values       \n",
    "                \n",
    "\n",
    "    def align_line_configuration_data(self, group_key, max_line_types=2, type_key=None):\n",
    "        \n",
    "        pdf_data_keys    = self.sections['equipment specifications'][f'{group_key} keys']\n",
    "        equipment_df = self.sections['equipment specifications']['data']\n",
    "        \n",
    "        if 'separated' in group_key:\n",
    "            storage_key = f\"{type_key.split(' ')[0]}_config\" \n",
    "            storage_data_keys = [self.sections['equipment specifications'][config_group_key] \n",
    "                                 for config_group_key in self.sections['equipment specifications'].keys() \n",
    "                                 if type_key in config_group_key][0]\n",
    "            configuration_cols = list()\n",
    "            for dKey in pdf_data_keys:\n",
    "                config_col_match_scores = [(col, levenshtein.normalized_similarity(col, dKey)) for col in equipment_df.columns]\n",
    "                scores = np.array([score for _, score in config_col_match_scores])\n",
    "                config_col = config_col_match_scores[np.argmax(scores)][0]    \n",
    "                configuration_cols.append(config_col)\n",
    "        else:\n",
    "            storage_key = f\"{group_key.split(' ')[0]}_config\"\n",
    "            storage_data_keys = pdf_data_keys\n",
    "            configuration_cols = [col for col in equipment_df if group_key in col.lower()]\n",
    "            \n",
    "        data_dict = dict()\n",
    "        for idx in range(max_line_types): \n",
    "            data_dict[str(idx)] = dict()\n",
    "            for dKey in storage_data_keys:\n",
    "                data_dict[str(idx)][dKey] = [None for k in range(equipment_df.shape[0])]\n",
    "\n",
    "        if 'separated' in group_key:\n",
    "            self.extract_line_config_data_from_separated_cells(equipment_df,\n",
    "                                                               configuration_cols,\n",
    "                                                               data_dict,\n",
    "                                                               pdf_data_keys,\n",
    "                                                               storage_key)\n",
    "        else:\n",
    "            self.extract_line_config_data_from_cells_containing_all_key_value_pairs(equipment_df, \n",
    "                                                                                    configuration_cols[0],\n",
    "                                                                                    data_dict, \n",
    "                                                                                    pdf_data_keys,\n",
    "                                                                                    storage_key,)\n",
    "           \n",
    "    def get_exhibit_name(self):\n",
    "        for text_container in self.text_containers[0]:\n",
    "            text = text_container.get_text()\n",
    "            if 'exhibit' in text.lower():\n",
    "                pattern = regex.compile(r'^\\s+')\n",
    "                exhibit = pattern.sub('', text.lower().replace('exhibit', '').replace('\\n', ''))\n",
    "                self.exhibit = exhibit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ocr_pdf_path  = Path('amendments/New_Exhibit_Redacted_docTR.pdf')\n",
    "new_orig_pdf_path = Path('amendments/New_Exhibit_Redacted.pdf')\n",
    "old_ocr_pdf_path  = Path('amendments/Old_Exhibit_Redacted_docTR.pdf')\n",
    "old_orig_pdf_path = Path('amendments/Old_Exhibit_Redacted.pdf')\n",
    "\n",
    "config_path = Path(r'C:\\Users\\Dalton\\Documents\\personal_records\\apex_consulting\\doctr_ocr\\configs\\atc_extra_info_config.yaml')\n",
    "\n",
    "det_arch_options  = ['linknet_resnet18',\n",
    "                     'linknet_resnet34',\n",
    "                     'linknet_resnet50',\n",
    "                     'db_resnet50',\n",
    "                     'db_mobilenet_v3_large',\n",
    "                     'fast_tiny',\n",
    "                     'fast_small',\n",
    "                     'fast_base',]\n",
    "\n",
    "reco_arch_options = ['crnn_vgg16_bn',\n",
    "                     'crnn_mobilenet_v3_small',\n",
    "                     'crnn_mobilenet_v3_large',\n",
    "                     'sar_resnet31',\n",
    "                     'master',\n",
    "                     'vitstr_small',\n",
    "                     'vitstr_base',\n",
    "                     'parseq',]\n",
    "\n",
    "predictor = ocr_predictor(det_arch='fast_base', reco_arch='crnn_vgg16_bn', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model # gps cma-b/6521/e0-6 atm1900d-1cwa radio 4478 b71 rru22 apxvaa24_43-u-\n",
      "dimensions 12\" x 9\" x 6\" 96\" x 24\" x 8.5\" 81.1\" x 7.7\" x 4.8\" 8.6\" x 10\" x 2.6\" 15\" x 13.2\" x 7.4\" 20.2\" x 13.2\" x 6.9\"\n",
      "azimuths/dir. of n/a 60/180/300 60/80/300 60/180/300 60/180/300 60\n",
      "quant. per n/a 1/1/1 2/2/2 1/1/1 1/1/1 1\n",
      "tx/rx frequency n/a mhz mhz n/a n/a mhz\n",
      "tx frequency n/a 668-688 n/a n/a 1930-1945,2145- 2145-2155,1735-\n",
      "rx frequency n/a 622-642 n/a n/a 1850-1865,1745- 1745-1755,2135-\n",
      "using unlicensed no no no no no no\n",
      "configuration qty: 1\n",
      "dimensions 20.2\" x 13.2\" x 6.9\" 6.23' x 6.23' x 4.32' 10' x -' x -' 9.1\" x 9.2\" x 3.9\" 9.1\" x 9.2\" x 3.9\" 4.23' x -' x -'\n",
      "azimuths/dir. of 180/300 292.15 184.14 292.15 184.14 292.15\n",
      "quant. per 1/1 1 1 4 2 1\n",
      "tx/rx frequency mhz ghz ghz n/a n/a ghz\n",
      "tx frequency 6 11 n/a n/a 6 2145-2155,1735-\n",
      "rx frequency 6 1 n/a n/a 6 1745-1755,2135-\n",
      "using unlicensed no no no no no no\n",
      "exhibit a-4\n",
      "ground space requirements\n",
      "total lease area sq.ft: 216.00 primary contiguous lease area l: 12.00' w: 18.00' h: 10.00' sq.ft: 216.00\n",
      "concrete pad 10.00' 16.00' n/a 160.00\n",
      "outside primary lease area n/a n/a n/a sq.ft: n/a\n",
      "backup power requirements\n",
      "generator: n/a fuel tank size (gal): n/a fuel type: n/a fuel tank setback (radius): n/a\n",
      "utility requirements\n",
      "power provided by: utility company direct\n",
      "telco/interconnect: n/a\n",
      "transmitter & receiver: specifications\n",
      "type: n/a quantity: n/a tx power (watts): n/a erp power (watts): n/a\n",
      "equipment specifications\n",
      "type gps panel panel tta rru/rrh rru/rrh\n",
      "manufacturer generic rfs cellmax rfs ericsson ericsson\n",
      "model # gps apxvaa24.43-u- cma-b/6521/e0-6 atm1900d-1cwa radio 4478 b71 rru22\n",
      "dimensions 12\"x9\"x6\" 96\" x 24\" x 8.5\" 81.1\"x7.7\": x4 4.8\" 8.6\" x 10\" x 2.6\" 15\" x 13.2\" x7.4\" 20.2\" x 13.2\" x6 6.9\"\n",
      "weight (ibs.) 10.0 101.4 35.0 8.4 60.0 52.9\n",
      "location ground tower tower tower tower tower\n",
      "rad center agl n/a 180.0' 180.0' 180.0' 180.0' 180.0'\n",
      "tip height n/a 184.0' 183.4' 180.4' 180.6' 180.8'\n",
      "base height n/a 176.0' 176.6' 179.6' 179.4' 179.2'\n",
      "mount type n/a side arm side arm side arm side arm side arm\n",
      "quantity 1 3 6 3 3 1\n",
      "azimuths/dir. of n/a 60/180/300 60/80/300 60/180/300 60/180/300 60\n",
      "quant. per n/a 1/1/1 2/2/2 1/1/1 1/1/1 1\n",
      "tx/rx frequency n/a mhz mhz n/a n/a mhz\n",
      "tx frequency n/a 668-688 1930-1945,2145- n/a n/a 2145-2155,1735-\n",
      "rxi frequency n/a 622-642 1850-1865,1745- n/a n/a 1745-1755,2135-\n",
      "using unlicensed no no no no no no\n",
      "antenna gain n/a 13.2/1 13.6 18.3/ 18.7/ 19.2 12 n/a n/a\n",
      "total # of lines n/a 3 6 1 3 2\n",
      "n/a qty: 3 qty: 6 qty: 1 qty: 3 n/a\n",
      "individual line diameter: 1/2\" coax diameter: 1 5/8\" diameter: 1 5/8\" diameter: 1/4\" coax\n",
      "configuration azimuth/sector: 1/1/1 coax (1.63'-41.3mm) fiber azimuth/sector: 1/1/1\n",
      "azimuth/sector: 2/2/2 azimuth/sector: 1/0/0\n",
      "n/a n/a n/a n/a nia qty: 1\n",
      "type: 2\" conduit\n",
      "containing:\n",
      "-\n",
      "azimuth/sector: 1\n",
      "conduit\n",
      "configuration qty: 1\n",
      "type: 2\" conduit\n",
      "containing:\n",
      "-\n",
      "azimuth/sector: 1\n",
      "equipment specifications\n",
      "type rru/rrh dish-hp dish-hp radio/odu radio/odu dish-hp\n",
      "manufacturer ericsson commscope commscope ceragon ceragon commscope\n",
      "model # rru22 usx6-6w usx10-11w rfu-d rfu-d shpx4-6w\n",
      "dimensions 20.2\" x 13.2\" x 6.9\" 6.23' x 6.23' x 4.32' 10' x-'x- - 9.1\" x 9.2\" x: 3.9\" 9.1\" x 9.2\" x: 3.9\" 4.23' x-'x-\n",
      "weight (ibs.) 52.9 198.0 579.8 14.3 14.3 70.5\n",
      "location tower tower tower tower tower tower\n",
      "rad center agl 180.0' 155.0' 155.0' 155.0' 155.0' 125.0'\n",
      "tip height 180.8' 158.1' 160.0' 155.4' 155.4' 127.1'\n",
      "base height 179.2' 151.9' 150.0' 154.6' 154.6' 122.9'\n",
      "mount type side arm pole mount pole mount pole mount pole mount pole mount\n",
      "quantity 2 1 1 4 2 1\n",
      "azimuths/dir. of 180/300 292.15 184.14 292.15 184.14 292.15\n",
      "quant. per 1/1 1 1 4 2 1\n",
      "tx/rx frequency mhz ghz ghz n/a n/a ghz\n",
      "txi frequency 2145-2155,1735- 6 11 nia n/a 6\n",
      "rx frequency 1745-1755,2135- 6 1 n/a n/a 6\n",
      "using unlicensed no no no no no no\n",
      "antenna gain n/a 38.3/3 38.8/ /3 39.3 n/a n/a n/a 35.5 dbi\n",
      "total # of lines 3 n/a n/a 8 4 n/a\n",
      "qty: 3 n/a n/a qty: 4 qty: 2 n/a\n",
      "type: hard line type: control cable type: control cable\n",
      "diameter: 1/8\" hard diameter: 0.31\" (7.8 diameter: 0.31\" (7.8\n",
      "azimuth/sector: 2/1 azimuth/sector: 4 azimuth/sector: 2\n",
      "individual line\n",
      "configuration\n",
      "qty: 4 qty: 2\n",
      "type: fiber/hybrid type: fiber/hybrid\n",
      "diameter: 0.19\" (4.8 diameter: 0.19\" (4.8\n",
      "mm) fiber mm) fiber\n",
      "azimuth/sector: 4 azimuth/sector: 2\n",
      "conduit n/a n/a n/a n/a n/a n/a\n",
      "configuration\n"
     ]
    }
   ],
   "source": [
    "new_exhibit = ATC_amendment(new_orig_pdf_path, new_ocr_pdf_path, config_path, ':')\n",
    "new_exhibit.get_text_lines_from_original()\n",
    "new_exhibit.run_ocr(predictor)\n",
    "\n",
    "new_exhibit.get_text_lines_from_ocr()\n",
    "new_exhibit.get_section_bounds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lease area sq.ft: 216.00\n",
      "primary contiguous lease area - l: 12.00'\n",
      "primary contiguous lease area - w: 18.00'\n",
      "primary contiguous lease area - h: 10.00'\n",
      "primary contiguous lease area - sq.ft: 216.00\n",
      "concrete pad - l: 10.00'\n",
      "concrete pad - w: 16.00'\n",
      "concrete pad - h: n/a\n",
      "concrete pad - sq.ft: 160.00\n",
      "outside primary lease area - l: n/a\n",
      "outside primary lease area - w: n/a\n",
      "outside primary lease area - h: n/a\n",
      "outside primary lease area - sq.ft: n/a\n",
      "generator: n/a\n",
      "fuel tank size (gal): n/a\n",
      "fuel type: n/a\n",
      "fuel tank setback (radius): n/a\n",
      "power provided by: utility company direct\n",
      "telco/interconnect: n/a\n",
      "type: n/a\n",
      "quantity: n/a\n",
      "tx power (watts): n/a\n",
      "erp power (watts): n/a\n"
     ]
    }
   ],
   "source": [
    "new_exhibit.fill_implicit_keys('ground space requirements', left_mult=2, right_mult=2)\n",
    "new_exhibit.extract_table_data()\n",
    "new_exhibit.extract_text_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qty: 3\n",
      "Type: Coax\n",
      "Diameter: 1/2\" Coax\n",
      "Azimuth/Sector: 1/1/1\n",
      "Qty: 6\n",
      "Type: Coax\n",
      "Diameter: 1 5/8\"Coax\n",
      "Azimuth/Sector: 2/2/2\n",
      "Qty: 1\n",
      "Type: Fiber/Hybrid\n",
      "Diameter: 1 5/8\"(1.63\"-41.3mm) Fiber\n",
      "Azimuth/Sector: 1/0/0\n",
      "Qty: 3\n",
      "Type: Coax\n",
      "Diameter: 1/4\" Coax\n",
      "Azimuth/Sector: 1/1/1\n",
      "Qty: 3\n",
      "Type: Hard Line\n",
      "Diameter: 1/8\" HardLine\n",
      "Azimuth/Sector: 2/1\n",
      "Qty: 4\n",
      "Qty: 4\n",
      "Type: Control Cable\n",
      "Type: Fiber/Hybrid\n",
      "Diameter: 0.31\" (7.8mm) Cable\n",
      "Diameter: 0.19\" (4.8mm) Fiber\n",
      "Azimuth/Sector: 4\n",
      "Azimuth/Sector: 4\n",
      "Qty: 2\n",
      "Qty: 2\n",
      "Type: Control Cable\n",
      "Type: Fiber/Hybrid\n",
      "Diameter: 0.31\" (7.8mm) Cable\n",
      "Diameter: 0.19\" (4.8mm) Fiber\n",
      "Azimuth/Sector: 2\n",
      "Azimuth/Sector: 2\n",
      "Qty: 1\n",
      "Qty: 1\n",
      "Type: 2\" conduit\n",
      "Type: 2\" conduit\n",
      "containing:-;\n",
      "containing:-;\n",
      "Azimuth/Sector: 1\n",
      "Azimuth/Sector: 1\n"
     ]
    }
   ],
   "source": [
    "new_exhibit.align_line_configuration_data(group_key = 'line configuration'   , max_line_types=2)\n",
    "new_exhibit.align_line_configuration_data(group_key = 'conduit configuration', max_line_types=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                                                       Radio/ODU\n",
       "Manufacturer                                                                 Ceragon\n",
       "Model #                                                                        RFU-D\n",
       "Dimensions HxWxD                                                  9.1\" x 9.2\" x 3.9\"\n",
       "Weight (lbs.)                                                                   14.3\n",
       "Location                                                                       Tower\n",
       "RAD Center AGL                                                                155.0'\n",
       "Tip Height                                                                    155.4'\n",
       "Base Height                                                                   154.6'\n",
       "Mount Type                                                                Pole Mount\n",
       "Quantity                                                                           4\n",
       "Azimuths/Dir. of Radiation                                                    292.15\n",
       "Quant. Per Azimuth/Sector                                                          4\n",
       "TX/RX Frequency Units                                                            N/A\n",
       "TX Frequency                                                                     N/A\n",
       "RX Frequency                                                                     N/A\n",
       "Using Unlicensed Frequencies?                                                     No\n",
       "Antenna Gain                                                                     N/A\n",
       "Total # of Lines                                                                   8\n",
       "Individual Line Configuration      Qty: 4\\nType: Control Cable\\nDiameter: 0.31\" (...\n",
       "Conduit Configuration                                                            N/A\n",
       "line_config_0_Qty                                                                  4\n",
       "line_config_0_Type                                                     Control Cable\n",
       "line_config_0_Diameter                                           0.31\" (7.8mm) Cable\n",
       "line_config_0_Azimuth/Sector                                                       4\n",
       "line_config_1_Qty                                                                  4\n",
       "line_config_1_Type                                                      Fiber/Hybrid\n",
       "line_config_1_Diameter                                           0.19\" (4.8mm) Fiber\n",
       "line_config_1_Azimuth/Sector                                                       4\n",
       "conduit_config_0_Qty                                                            None\n",
       "conduit_config_0_Type                                                           None\n",
       "conduit_config_0_containing                                                     None\n",
       "conduit_config_0_Azimuth/Sector                                                 None\n",
       "conduit_config_1_Qty                                                            None\n",
       "conduit_config_1_Type                                                           None\n",
       "conduit_config_1_containing                                                     None\n",
       "conduit_config_1_Azimuth/Sector                                                 None\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_exhibit.sections['ground space requirements']['data']\n",
    "new_exhibit.sections['backup power requirements']['data']\n",
    "new_exhibit.sections['utility requirements']['data']\n",
    "new_exhibit.sections['transmitter & receiver specifications']['data']\n",
    "new_exhibit.sections['equipment specifications']['data'].iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equipment tip 158.1' 160.0' 155.4' 155.4' 127.1' 125.4'\n",
      "equipment base 151.9' 150.0' 154.6' 154.6' 122.9' 124.6'\n",
      "azimuths/dir. of 292.15 184.14 184.14 292.15 292.15 292.15\n",
      "quant. per 1 1 2 1 1 1\n",
      "tx/rx frequency mhz ghz n/a n/a mhz n/a\n",
      "using unlicensed no no no no no no\n",
      "line quant. per 1 n/a see config. summary n/a 1 n/a\n",
      "exhibit a-3\n",
      "nmmu4u4ub\n",
      "ground space requirements\n",
      "total lease area sq. ft: 216.00' primary contiquous lease area l:12.00' w:18.00' h:10.00 sq. ft: 216.00\n",
      "concrete pad 10.00' 16.00' n/a 160.00\n",
      "outside primary lease area nia nia n/a sq. ft: nia\n",
      "backup power requirements\n",
      "generator: nia capacity(kw): nia fuel tank size(gal): n/a fuel type: nia fuel tank setback/radius): n/a\n",
      "utility requirements\n",
      "power provided by: utility company direct\n",
      "telcolinterconnect: nia\n",
      "transmitter & receiver specifications\n",
      "type: n/a quantity: n/a txi power(watts): n/a erp(watts): n/a\n",
      "equipment specifications\n",
      "type panel panel tta rru/rrh rru/rrh rru/rrh\n",
      "manufacturer cellmax rfs rfs ericsson ericsson ericsson\n",
      "model! # cma-b/6521/e0-6 apxvaa24.43-u/a20) atm1900d-1cwa rru22 rru22 radio 4478 b71\n",
      "dimensions hxwxd 81.1'x7.7x4.8 96\" x 24\"x8.5\" 8.6\" x 10\" x2.6\" 20.2\" x 13.2\"x6.9\" 20.2\" x 13.2\"x6.9\" 15\" x 13.2\"x7.4\"\n",
      "weight(lbs.) 35.0 101.4 8.4 52.9 52.9 60.0\n",
      "location tower tower tower tower tower tower\n",
      "rad center agl 180.0' 180.0' 180.0' 180.0' 180.0\" 180.0'\n",
      "equipment height tip 183.4\" 184.0' 180.4\" 180.8\" 180.8 180.6'\n",
      "height equipment base 176.6' 176.0' 179.6 179.2\" 179.2\" 179.4\"\n",
      "mount type side arm side arm side arm side arm side arm side arm\n",
      "quantity 6 3 3 1 2 3\n",
      "azimuths/dir. of 60/80/300 60/180/300 60/180/300 60 180/300 60/180/300\n",
      "azimuth/sector quant. per nia 1/1/1 1/1/1 1 1/1 1/1/1\n",
      "tx/rx units frequency mhz mhz n/a mhz mhz n/a\n",
      "tx frequency 1930-1945,2145- 668-688 n/a 2145-2155,1735- 1740 2145-2155,1735- n/a\n",
      "rxf frequency 1850-1865,1745- 622-642 n/a 1745-1755,2135- 2140 1745-1755,2135- 2140 n/a\n",
      "using frequencies? unlicensed no no no no no no\n",
      "equipment gain 18.3/ 18.71 19.2 13.2/13.6 12 n/a n/a n/a\n",
      "total # ofl lines 6 3 1 2 3 3\n",
      "line quant. per n/a 1/1/1 1/0/0 2 2/1 1/1/1\n",
      "line type coax coax fiber/hybrid conduit hard line coax\n",
      "line diameter size 1 5/8\" coax 1/2\" coax 1 5/8\" (1.63'-41.3mm) fiber 2\" conduit 1/8\" hard line 1/4\" coax\n",
      "line configuration nia nia nia nia nia nia\n",
      "equipment specifications\n",
      "type dish-hp dish-hp radio/odu radio/odu dish-hp radio/odu\n",
      "manufacturer rfs commscope ceragon ceragon rfs ceragon\n",
      "model # sb6-w60bc usx10-11w rfu-d fibeair ip-20e sb4-w60 fibeair ip-20e\n",
      "dimensions! hxwxd 6.23' x6 6.23' x2.98' 10.00' x-'x- 9.1\"x9.2'x3.9\" 9.2\"x 9.1\"x3.9\" 4.14' x4 4.14' x- 9.2\": x9.1\"x3.9\"\n",
      "weight(lbs.) 198.0 579.8 14.3 14.3 77.0 14.3\n",
      "location tower tower tower tower tower tower\n",
      "rad center agl 155.0' 155.0' 155.0' 155.0' 125.0' 125.0'\n",
      "height equipment tip 158.1\" 160.0' 155.4' 155.4' 127.1' 125.4'\n",
      "height equipment base 151.9' 150.0' 154.6' 154.6' 122.9 124.6'\n",
      "mount type pole mount pole mount polel mount pole mount pole mount polel mount\n",
      "quantity 1 1 2 1 1 1\n",
      "azimuths/dir. of 292.15 184.14 184.14 292.15 292.15 292.15\n",
      "quant. per 1 1 2 1 1 1\n",
      "tx/rxf frequency mhz ghz n/a n/a mhz n/a\n",
      "txi frequency 6400 11 n/a n/a 6400 n/a\n",
      "rxi frequency 6400 1 n/a n/a 6400 n/a\n",
      "using frequencies? unlicensed no no no no no no\n",
      "equipment gain 35.7/: 36.7/ 37.3 n/a n/a n/a 32.4/ /32.7/33.4 n/a\n",
      "total # of lines 1 0 4 0 1 0\n",
      "line quant. per 1 n/a see config. summary n/a 1 n/a\n",
      "line type coax n/a multiple n/a coax n/a\n",
      "line diameter size 0.29\" (7.2mm) rg-8 n/a see config. summary n/a 0.29\" (7.2mm) rg-8 n/a\n",
      "2- control cable;\n"
     ]
    }
   ],
   "source": [
    "old_exhibit = ATC_amendment(old_orig_pdf_path, old_ocr_pdf_path, config_path, ':')\n",
    "old_exhibit.get_text_lines_from_original()\n",
    "old_exhibit.run_ocr(predictor)\n",
    "\n",
    "old_exhibit.get_text_lines_from_ocr()\n",
    "old_exhibit.get_section_bounds()\n",
    "\n",
    "old_exhibit.fill_implicit_keys('ground space requirements', left_mult=2, right_mult=2)\n",
    "old_exhibit.extract_table_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lease area sq. ft: 216.00'\n",
      "primary contiquous lease area - l: 12.00'\n",
      "primary contiquous lease area - w: 18.00'\n",
      "primary contiquous lease area - h: 10.00\n",
      "primary contiquous lease area - sq. ft: 216.00\n",
      "concrete pad - l: 10.00'\n",
      "concrete pad - w: 16.00'\n",
      "concrete pad - h: n/a\n",
      "concrete pad - sq. ft: 160.00\n",
      "outside primary lease area - l: nia\n",
      "outside primary lease area - w: nia\n",
      "outside primary lease area - h: n/a\n",
      "outside primary lease area - sq. ft: nia\n",
      "generator: nia\n",
      "capacity(kw): nia\n",
      "fuel tank size(gal): n/a\n",
      "fuel type: nia\n",
      "fuel tank setback/radius): n/a\n",
      "power provided by: utility company direct\n",
      "telcolinterconnect: nia\n",
      "type: n/a\n",
      "quantity: n/a\n",
      "txi power(watts): n/a\n",
      "erp(watts): n/a\n",
      "type Type\n",
      "manufacturer Manufacturer\n",
      "model! # Model #\n",
      "dimensions hxwxd Dimensions HxWxD\n",
      "weight(lbs.) Weight(lbs.)\n",
      "equipment tip height Equipment Tip Height\n",
      "equipment base height Equipment Base Height\n",
      "mount type Mount Type\n",
      "azimuths/dir. of radiation Azimuths/Dir. of Radiation\n",
      "quant. per azimuth/sector Quant. Per Azimuth/Sector\n",
      "tx frequency TX Frequency\n",
      "rxf frequency RX Frequency\n",
      "equipment gain Equipment Gain\n",
      "total lines # ofl Total # of Lines\n",
      "line quant. per azimuth/sector Line Quant. Per Azimuth/Sector\n",
      "line type Line Type\n",
      "line diameter size Line Diameter Size\n"
     ]
    }
   ],
   "source": [
    "old_exhibit.extract_text_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of tokens in line_config is not a multiple of the expected number of keys for storing data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mold_exhibit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_line_configuration_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseparated configuration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_line_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m old_exhibit\u001b[38;5;241m.\u001b[39malign_line_configuration_data(group_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseparated configuration\u001b[39m\u001b[38;5;124m'\u001b[39m, max_line_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, type_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconduit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[96], line 966\u001b[0m, in \u001b[0;36mATC_amendment.align_line_configuration_data\u001b[1;34m(self, group_key, max_line_types, type_key)\u001b[0m\n\u001b[0;32m    963\u001b[0m         data_dict[\u001b[38;5;28mstr\u001b[39m(idx)][dKey] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(equipment_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseparated\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m group_key:\n\u001b[1;32m--> 966\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_line_config_data_from_separated_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequipment_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mconfiguration_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mpdf_data_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mstorage_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_line_config_data_from_cells_containing_all_key_value_pairs(equipment_df, \n\u001b[0;32m    973\u001b[0m                                                                             configuration_cols[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    974\u001b[0m                                                                             data_dict, \n\u001b[0;32m    975\u001b[0m                                                                             pdf_data_keys,\n\u001b[0;32m    976\u001b[0m                                                                             storage_key,)\n",
      "Cell \u001b[1;32mIn[96], line 916\u001b[0m, in \u001b[0;36mATC_amendment.extract_line_config_data_from_separated_cells\u001b[1;34m(self, equipment_df, configuration_cols, data_dict, data_keys, storage_key)\u001b[0m\n\u001b[0;32m    914\u001b[0m nKeys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line_config) \u001b[38;5;241m%\u001b[39m nKeys \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of tokens in line_config is not a multiple of the expected number of keys for storing data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    918\u001b[0m equip_idx_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(idx\u001b[38;5;241m/\u001b[39mnKeys) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(line_config))]\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dKey, token, equipIdx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys(), line_config, equip_idx_list):\n",
      "\u001b[1;31mValueError\u001b[0m: The number of tokens in line_config is not a multiple of the expected number of keys for storing data"
     ]
    }
   ],
   "source": [
    "old_exhibit.align_line_configuration_data(group_key = 'separated configuration', max_line_types=2, type_key = 'line')\n",
    "# old_exhibit.align_line_configuration_data(group_key = 'separated configuration', max_line_types=2, type_key = 'conduit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  data_not_found\n",
       "Type                                                       panel\n",
       "Manufacturer                                             cellmax\n",
       "Model #                                          cma-b/6521/e0-6\n",
       "Dimensions HxWxD                                   81.1'x7.7x4.8\n",
       "Weight(lbs.)                                                35.0\n",
       "Location                                          data_not_found\n",
       "RAD Center AGL                                    data_not_found\n",
       "Equipment Tip Height                                      183.4\"\n",
       "Equipment Base Height                                     176.6'\n",
       "Mount Type                                              side arm\n",
       "Quantity                                          data_not_found\n",
       "Azimuths/Dir. of Radiation                             60/80/300\n",
       "Quant. Per Azimuth/Sector                                    nia\n",
       "TX/RX Frequency Units                             data_not_found\n",
       "TX Frequency                      1930-1945,2145- 2155,1735-1740\n",
       "RX Frequency                      1850-1865,1745- 1755,2135-2140\n",
       "Using Unlicensed Frequencies?                     data_not_found\n",
       "Equipment Gain                                  18.3/ 18.71 19.2\n",
       "Total # of Lines                                               6\n",
       "Line Quant. Per Azimuth/Sector                               n/a\n",
       "Line Type                                                   coax\n",
       "Line Diameter Size                                   coax 1 5/8\"\n",
       "Line Configuration                                data_not_found\n",
       "Name: 6, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_exhibit.sections['ground space requirements']['data']\n",
    "old_exhibit.sections['backup power requirements']['data']\n",
    "old_exhibit.sections['utility requirements']['data']\n",
    "old_exhibit.sections['transmitter & receiver specifications']['data']\n",
    "old_exhibit.sections['equipment specifications']['data'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
