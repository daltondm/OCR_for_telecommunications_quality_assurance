{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "1. Extract/organize line configuration and conduit configuration data from old_exhibit\n",
    "2. Write pdf_comparison class\n",
    "3. Write pdf_markup class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (20231228)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfminer.six) (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (0.11.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfplumber) (10.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Requirement already satisfied: numpy in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: textdistance in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (4.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: regex in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (2024.5.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dalton\\documents\\personal_records\\apex_consulting\\doctr_ocr\\.conda\\lib\\site-packages (6.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfminer.six\n",
    "%pip install pdfplumber\n",
    "%pip install numpy pandas\n",
    "%pip install textdistance\n",
    "%pip install regex\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipympl\n",
    "%pip install reportlab>=3.6.2\n",
    "%pip install PyPDF2\n",
    "%pip install ocrmypdf\n",
    "%pip install pdf2jpg\n",
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages, extract_text\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTRect, LTFigure\n",
    "# To extract text from tables in PDF\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Iterable, Dict, Tuple, ClassVar\n",
    "import regex\n",
    "from textdistance import hamming, jaro, levenshtein\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "os.environ['USE_TORCH'] = '1'\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfMerger\n",
    "from ocrmypdf.hocrtransform import HocrTransform\n",
    "# import fitz\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdf_data():\n",
    "    def __init__(self,   \n",
    "                 orig_filepath: str | Path, \n",
    "                 ocr_filepath:  str | Path, \n",
    "                 config:        str | Path,\n",
    "                 key_val_sep:   str = ':' ,) -> None:\n",
    "        self.orig_filepath = orig_filepath\n",
    "        self.ocr_filepath = ocr_filepath\n",
    "        self.config = config  \n",
    "        self.text_df = None  \n",
    "        self.px_col_sep = 8\n",
    "        self.px_word_sep = 2\n",
    "        self.col_sep_str = ' | '\n",
    "        self.key_val_sep = key_val_sep\n",
    "\n",
    "    def combine_key_value_pairs_in_words_df(self, words_df):\n",
    "        drop_idxs = list()\n",
    "        for wIdx, word in words_df.iterrows():\n",
    "            if 'source' in words_df.columns and any(words_df['source'] == 'ocr'):\n",
    "                test = (word['text'][-1] == self.key_val_sep) and \\\n",
    "                       (wIdx != words_df.index[-1])  \n",
    "            else:\n",
    "                test = (word['text'][-1] == self.key_val_sep) and \\\n",
    "                       (wIdx != words_df.index[-1]) and \\\n",
    "                       (words_df.loc[wIdx+1, 'top'] == word['top'])  \n",
    "\n",
    "            if test:\n",
    "                drop_idxs.append(wIdx+1)\n",
    "                words_df.loc[wIdx, 'text'] = f\"{word['text']}{words_df.loc[wIdx+1, 'text']}\"  \n",
    "                words_df.loc[wIdx, 'right'] = words_df.loc[wIdx+1, 'right']       \n",
    "        words_df = words_df.drop(drop_idxs).reset_index(drop=True)    \n",
    "\n",
    "        return words_df\n",
    "\n",
    "    def fill_implicit_keys(self, section_header, left_mult=2, right_mult=2):\n",
    "        section_dict = self.sections[section_header]\n",
    "        for sub_idx, subsection_bounds in section_dict['bounds'].iterrows():    \n",
    "            mask = (self.text_df['norm_top'   ] > subsection_bounds['top'   ]) & \\\n",
    "                   (self.text_df['norm_bottom'] < subsection_bounds['bottom']) \n",
    "            subsect_df = self.text_df.loc[mask, :]\n",
    "\n",
    "            is_ocr = True if any(subsect_df['source'] == 'ocr') else False\n",
    "            if is_ocr:\n",
    "                # page = pdfplumber.open(self.ocr_filepath ).pages[subsect_df.loc[subsect_df.index[0], 'page']]\n",
    "                words_df = self.ocr_words_df.copy()\n",
    "                words_mask = (words_df['norm_top'   ] > subsection_bounds['top'   ]) & \\\n",
    "                             (words_df['norm_bottom'] < subsection_bounds['bottom']) \n",
    "                words_df = words_df.loc[words_mask, :]\n",
    "                merged_rows = [idx for idx, word in words_df.iterrows() if ' ' in word['text']]\n",
    "                for idx in merged_rows:\n",
    "                    words_df.loc[idx, 'text'] = words_df.loc[idx, 'text'].split(' ')[0] \n",
    "                words_df['top'] = round(words_df['top'], -1)\n",
    "            else:\n",
    "                page = pdfplumber.open(self.orig_filepath).pages[subsect_df.loc[subsect_df.index[0], 'page']]\n",
    "                words_df = self.get_words_df(page, subsect_df['top'].min(), subsect_df['bottom'].max())\n",
    "                words_df.loc[:, 'top'] = np.round(words_df['top']) \n",
    "            \n",
    "\n",
    "            words_df.sort_values(by=['top', 'left'], ignore_index=True, inplace=True)   \n",
    "\n",
    "            words_df = self.combine_key_value_pairs_in_words_df(words_df)\n",
    "\n",
    "            split_lines     = list()\n",
    "            split_lines_row = list()\n",
    "            split_lines_col = list()\n",
    "            drop_idxs = list()\n",
    "            for rIdx, line in subsect_df.iterrows():\n",
    "                text_by_col = line['text'].split(self.col_sep_str)\n",
    "                split_lines.extend(text_by_col)\n",
    "                split_lines_row.extend([rIdx for k in range(len(text_by_col))])\n",
    "                split_lines_col.extend(list(range(len(text_by_col))))\n",
    "                \n",
    "                # combine words_df to match phrases in split_lines\n",
    "                for tIdx, token in enumerate(text_by_col):\n",
    "                    combine_idxs = [idx for idx, word in words_df.iterrows() \n",
    "                                    if  word['text'] in token\n",
    "                                    and word['top']+0.5 >= line['top']\n",
    "                                    and word['bottom'] <= line['bottom']+5]\n",
    "                    tmp_df = words_df.loc[combine_idxs, :]\n",
    "                    \n",
    "                    first_word = token.split(' ')[0]\n",
    "                    possible_start_idx = tmp_df.index[tmp_df['text'] == first_word]\n",
    "                    for start_idx in possible_start_idx:\n",
    "                        phrase_idxs = range(start_idx, start_idx+len(token.split(' ')))\n",
    "                        if all([True if idx in tmp_df.index else False for idx in phrase_idxs]): \n",
    "                            phrase = ' '.join(tmp_df.loc[phrase_idxs, 'text'])\n",
    "                            if phrase == token:\n",
    "                                words_df.loc[start_idx, ['text', 'right']] = [phrase, tmp_df.loc[phrase_idxs[-1], 'right']]\n",
    "                                drop_idxs.extend(phrase_idxs[1:])             \n",
    "                                break \n",
    "\n",
    "            words_df = words_df.drop(drop_idxs).reset_index(drop=True)\n",
    "\n",
    "            if not all([True if phrase==token else False for phrase, token in zip(split_lines, words_df['text'])]):\n",
    "                raise ValueError('the phrases in split_lines do not match the corrected tokens in words_df')\n",
    "\n",
    "            for token, rIdx, cIdx, (wIdx, word) in zip(split_lines, split_lines_row, split_lines_col, words_df.iterrows()):\n",
    "                if self.key_val_sep in token:\n",
    "                    continue\n",
    "                token_bounds = word[['left', 'right']]\n",
    "                col_mask = ((words_df['right' ]- token_bounds['left'] > -left_mult*self.px_col_sep) & \\\n",
    "                            (words_df['left' ] - token_bounds['left'] <=  0                )) | \\\n",
    "                           ((words_df['right'] - token_bounds['right'] < right_mult*self.px_col_sep) & \\\n",
    "                            (words_df['right'] - token_bounds['right'] >= 0                ))\n",
    "                same_column_tokens = words_df.loc[col_mask, 'text']\n",
    "                \n",
    "                implicit_key = [item.split(self.key_val_sep)[0] for item in same_column_tokens.values if len(item.split(self.key_val_sep)) == 2]\n",
    "                if len(implicit_key) >= 1:\n",
    "                    original_text = self.text_df.loc[rIdx, 'text']\n",
    "                    text_cols = original_text.split(self.col_sep_str)\n",
    "                    text_cols[cIdx] = f'{implicit_key[0]}{self.key_val_sep}{text_cols[cIdx]}'\n",
    "                    self.text_df.loc[rIdx, 'text'] = self.col_sep_str.join(text_cols)  \n",
    "\n",
    "    def identify_line_merge_sets(self, text_df=None):\n",
    "\n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "        \n",
    "        is_ocr = True if any(text_df['source'] == 'ocr') else False\n",
    "\n",
    "        merge_sets = list()\n",
    "        for idx, line in text_df.iterrows():\n",
    "            if 'using' in line.text:\n",
    "                stop = []\n",
    "            if is_ocr:\n",
    "                same_line_overlap_thresh = 0.0048\n",
    "                # # top of word begins above bottom of current word, by at least 0.005 (.5% of page), and starts below top of word  \n",
    "                # merge_set = np.where((text_df['norm_top']    < line['norm_bottom']) &\n",
    "                #                     (np.abs(text_df['norm_top'] - line['norm_bottom']) > same_line_overlap_thresh) &\n",
    "                #                     (text_df['norm_top']   >= line['norm_top']   )  )[0]\n",
    "                # words that are below current word and overlap by at least same_line_overlap_thresh, or above current word and overlap by at least same \n",
    "                merge_set = np.where(((text_df['norm_top'] - line['norm_bottom'] < -same_line_overlap_thresh) &\n",
    "                                      (text_df['norm_top'] >= line['norm_top'])                           \n",
    "                                      ) |\n",
    "                                     ((text_df['norm_bottom'] - line['norm_top'] >  same_line_overlap_thresh) & \n",
    "                                      (text_df['norm_top'] <= line['norm_top'])))\n",
    "            else:\n",
    "                merge_set = np.where((text_df['norm_top']    < line['norm_bottom']) &\n",
    "                                    (text_df['norm_top']    > line['norm_top']   )  )[0]\n",
    "                \n",
    "            merge_set = text_df.index[merge_set]\n",
    "            if len(merge_set) > 0:\n",
    "                merge_set = merge_set.to_list()\n",
    "                merge_set = sorted(merge_set + [idx]) if idx not in merge_set else merge_set \n",
    "                same_merge_set   = any([True if m_set == merge_set else False for m_set in merge_sets])\n",
    "                overlapping_sets = [set_idx for set_idx, m_set in enumerate(merge_sets) if any(i for i in m_set if i in merge_set)]\n",
    "                if same_merge_set:\n",
    "                    continue\n",
    "                elif len(overlapping_sets) == 1:\n",
    "                    merge_sets[overlapping_sets[0]] = np.unique(merge_sets[overlapping_sets[0]] + merge_set).tolist() \n",
    "                elif len(overlapping_sets) > 1:\n",
    "                    raise ValueError('Have not written code to manage more than one overlapping set when combining lines')\n",
    "                else:\n",
    "                    merge_sets.append(merge_set)\n",
    "                    # print(' '.join(text_df.loc[merge_set,['text','left']].sort_values(by='left').text))\n",
    "        \n",
    "        text_df['line_idx'] = np.full((text_df.shape[0],), -1, dtype=int)\n",
    "        for msIdx, merge_set in enumerate(merge_sets):\n",
    "            text_df.loc[merge_set, 'line_idx'] = np.repeat(msIdx, len(merge_set))\n",
    "\n",
    "        return merge_sets, text_df\n",
    "        \n",
    "    def get_words_df(self, page, top, bottom):\n",
    "        page_crop = page.within_bbox((         0, top, \n",
    "                                        page.width, bottom))  \n",
    "\n",
    "        words = page_crop.extract_words()  \n",
    "        words_dict = dict(text=[], left=[], right=[], top=[], bottom=[])\n",
    "        for word in words:\n",
    "            if word['text'] == '|':\n",
    "                continue\n",
    "            word['text'] = word['text'].lower().replace('|','')\n",
    "            for dict_key, word_key in zip(['text', 'left', 'right', 'top', 'bottom'],\n",
    "                                            ['text',   'x0',    'x1', 'top', 'bottom']):\n",
    "                words_dict[dict_key].append(word[word_key])\n",
    "        \n",
    "        words_df = pd.DataFrame.from_dict(words_dict)\n",
    "        words_df.sort_values(by='left', ignore_index=True, inplace=True)\n",
    "\n",
    "        return words_df\n",
    "\n",
    "    def identify_columns_from_words_df(self, words_df):\n",
    "        col_id = []\n",
    "        col_num = 0\n",
    "        prev_w_info = None\n",
    "        for w_idx, w_info in words_df.iterrows():\n",
    "            if prev_w_info is not None: \n",
    "                if (w_info['left'] - prev_w_info['right'] > self.px_col_sep):\n",
    "                    col_num += 1\n",
    "                elif (w_info['left'] - prev_w_info['right'] < 0): \n",
    "                    w_info['right'] = max([prev_w_info['right'], w_info['right']])\n",
    "                # FIXME may need to reinstate this condition for original text\n",
    "                # elif (w_info['left'] - prev_w_info['right'] < 0): \n",
    "                #     w_info['right'] = prev_w_info['right'] \n",
    "            col_id.append(col_num)\n",
    "            prev_w_info = w_info.copy()\n",
    "        words_df['col_id'] = col_id\n",
    "        if 'source' in words_df.columns and any(words_df['source'] == 'ocr'):\n",
    "            # split top/bottom lines correctly within columns\n",
    "            avg_word_height = (words_df['bottom'] - words_df['top']).mean()\n",
    "            for col_id in words_df['col_id'].unique():\n",
    "                col_df = words_df.loc[words_df['col_id'] == col_id, :]\n",
    "                already_adjusted = []\n",
    "                for w_idx, w_info in col_df.iterrows():\n",
    "                    if w_idx not in already_adjusted:\n",
    "                        same_line_mask = abs(col_df['top'] - w_info['top']) < 0.33*avg_word_height\n",
    "                        same_line_idx = col_df.index[same_line_mask].values \n",
    "                        already_adjusted.extend(same_line_idx)\n",
    "                        words_df.loc[same_line_idx, 'top'] = round(col_df.loc[same_line_mask, 'top'].min())\n",
    "\n",
    "        words_df.sort_values(by=['col_id', 'top', 'left'], ignore_index=True, inplace=True)\n",
    "\n",
    "        col_phrases = []\n",
    "        for col_id in words_df['col_id'].unique():\n",
    "            col_df = words_df.loc[words_df['col_id'] == col_id, :]\n",
    "            col_phrases.append(' '.join(col_df['text']))\n",
    "\n",
    "        return col_phrases \n",
    "\n",
    "    def organize_single_lines(self, merge_sets, ocr_pdf, orig_pdf, text_df=None):\n",
    "        \n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "\n",
    "        merge_list = list(itertools.chain.from_iterable(merge_sets))\n",
    "        for idx, line in text_df.iterrows():\n",
    "            if idx not in merge_list:   \n",
    "                if line['source'] == 'ocr':\n",
    "                    page = ocr_pdf.pages [line['page']]\n",
    "                else:\n",
    "                    page = orig_pdf.pages[line['page']]\n",
    "\n",
    "                words_df = self.get_words_df(page, line['top'], line['bottom'])\n",
    "                words_df.loc[:, 'top'] = np.round(words_df['top']) \n",
    "\n",
    "                words_df = self.combine_key_value_pairs_in_words_df(words_df)\n",
    "\n",
    "                col_phrases = self.identify_columns_from_words_df(words_df)\n",
    "\n",
    "                text_df.loc[idx, 'text'] = self.col_sep_str.join(col_phrases)\n",
    "        return text_df\n",
    "\n",
    "    def condense_merge_sets(self, text_df = None, ocr_pdf = None, orig_pdf = None):\n",
    "\n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "        is_ocr = True if any(text_df['source'] == 'ocr') else False\n",
    "\n",
    "        drop_idxs = list()\n",
    "        for line_idx in text_df['line_idx'].unique():\n",
    "            if line_idx == -1:\n",
    "                continue\n",
    "            line_df = text_df.loc[text_df['line_idx'] == line_idx, :]\n",
    "            top     = line_df['top'].min()    \n",
    "            bottom  = line_df['bottom'].max()\n",
    "\n",
    "            if not is_ocr:\n",
    "                page = orig_pdf.pages[line_df.loc[line_df.index[0], 'page']]\n",
    "                words_df = self.get_words_df(page, top, bottom)\n",
    "                col_phrases = self.identify_columns_from_words_df(words_df)\n",
    "                # replace first line in merge set with merged text and position info, then store indices of \n",
    "                # remaining merge set to drop at end of combine method\n",
    "                text_df.loc[line_df.index[0], \n",
    "                            ['text','bottom','left','right', 'norm_bottom','top','norm_top',]\n",
    "                            ] = [self.col_sep_str.join(col_phrases),\n",
    "                                line_df['bottom'].max(),\n",
    "                                line_df['left'].min(),\n",
    "                                line_df['right'].max(),\n",
    "                                line_df['norm_bottom'].max(),\n",
    "                                line_df['top'].min(),\n",
    "                                line_df['norm_top'].min(),] \n",
    "\n",
    "            drop_idxs.extend(line_df.index[1:]) \n",
    "\n",
    "        text_df = text_df.drop(drop_idxs).reset_index(drop=True)\n",
    "        return text_df\n",
    "\n",
    "\n",
    "    def organize_text_lines_by_row_and_column(self, text_df=None):\n",
    "        merge_sets, text_df = self.identify_line_merge_sets(text_df)\n",
    "        orig_pdf = pdfplumber.open(self.orig_filepath) \n",
    "        text_df = self.organize_single_lines(merge_sets, None, orig_pdf, text_df)\n",
    "        text_df = self.condense_merge_sets   (text_df, None, orig_pdf)\n",
    "        return text_df\n",
    "    \n",
    "    def split_text_lines_with_pdfplumber(self, pdf, element, page_idx, text_dict,):\n",
    "        did_split = False\n",
    "        page = pdf.pages[page_idx]\n",
    "        page_crop = page.within_bbox((         0, page.height - element.y1, \n",
    "                                      page.width, page.height - element.y0))  \n",
    "\n",
    "        text = page_crop.extract_text_lines()\n",
    "        \n",
    "        if len(text) > 1:\n",
    "            did_split = True\n",
    "            for line in text:\n",
    "\n",
    "                norm_top    = page_idx + (line['top'   ] / page.height)\n",
    "                norm_bottom = page_idx + (line['bottom'] / page.height)\n",
    "                if np.logical_not(np.any((np.isclose(self.text_df['norm_top'   ], norm_top   , atol=1e-2)) &\n",
    "                                         (np.isclose(self.text_df['norm_bottom'], norm_bottom, atol=1e-2))  )):\n",
    "                    text_dict['page'       ].append(page_idx)\n",
    "                    text_dict['bottom'     ].append(line['bottom'])\n",
    "                    text_dict['top'        ].append(line['top'])\n",
    "                    text_dict['left'       ].append(line['x0'])\n",
    "                    text_dict['right'      ].append(line['x1'])\n",
    "                    text_dict['text'       ].append(line['text'].lower().replace('|', ''))\n",
    "                    text_dict['norm_top'   ].append(norm_top)\n",
    "                    text_dict['norm_bottom'].append(norm_bottom)\n",
    "                    text_dict['source'     ].append('ocr')\n",
    "                    \n",
    "                    print(f'\\nstoring text from OCR on page {page_idx}:\\n\"{line['text'].lower()}\"')\n",
    "\n",
    "        return did_split    \n",
    "\n",
    "    def get_text_lines_from_original(self):\n",
    "        pdf = pdfplumber.open(self.orig_filepath)\n",
    "        text_dict = dict(text=[], source=[], page=[], top=[], bottom=[], left=[], right=[], norm_top=[], norm_bottom=[], confidence=[])\n",
    "        for page_idx, page in enumerate(pdf.pages):\n",
    "            text_lines = page.extract_text_lines()\n",
    "            for line in text_lines:\n",
    "                norm_top    = page_idx + (line['top'   ] / page.height)\n",
    "                norm_bottom = page_idx + (line['bottom'] / page.height)\n",
    "\n",
    "                text_dict['page'       ].append(page_idx)\n",
    "                text_dict['bottom'     ].append(line['bottom'])\n",
    "                text_dict['top'        ].append(line['top'])\n",
    "                text_dict['left'       ].append(line['x0'])\n",
    "                text_dict['right'      ].append(line['x1'])\n",
    "                text_dict['text'       ].append(line['text'].lower().replace('|', ''))\n",
    "                text_dict['norm_top'   ].append(norm_top)\n",
    "                text_dict['norm_bottom'].append(norm_bottom)\n",
    "                text_dict['source'     ].append('original')\n",
    "                text_dict['confidence' ].append(1.0)\n",
    "        \n",
    "        text_df = pd.DataFrame(text_dict)\n",
    "        text_df = self.organize_text_lines_by_row_and_column(text_df=text_df)\n",
    "\n",
    "        if self.text_df is None:\n",
    "            self.text_df = text_df\n",
    "        elif len(text_df) > 0:\n",
    "            self.add_new_data_to_text_df(text_df)\n",
    "\n",
    "        self.text_df.sort_values(by='norm_top', inplace=True, ignore_index=True, ascending=True)\n",
    "\n",
    "    def identify_columns_from_words_df_docTR(self, words_df):\n",
    "        col_id = []\n",
    "        col_num = 0\n",
    "        prev_w_info = None\n",
    "        for w_idx, w_info in words_df.iterrows():\n",
    "            if prev_w_info is not None: \n",
    "                if (w_info['left'] - prev_w_info['right'] > self.px_col_sep):\n",
    "                    col_num += 1\n",
    "                elif (w_info['left'] - prev_w_info['right'] < 0):\n",
    "                    w_info['right'] = prev_w_info['right'] \n",
    "            col_id.append(col_num)\n",
    "            prev_w_info = w_info.copy()\n",
    "        words_df['col_id'] = col_id\n",
    "        words_df.sort_values(by=['col_id', 'top', 'left'], ignore_index=True, inplace=True)\n",
    "\n",
    "        col_phrases = []\n",
    "        for col_id in words_df['col_id'].unique():\n",
    "            col_df = words_df.loc[words_df['col_id'] == col_id, :]\n",
    "            col_phrases.append(' '.join(col_df['text']))\n",
    "\n",
    "        return col_phrases \n",
    "\n",
    "    def organize_single_lines_docTR(self, text_df=None):\n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "        \n",
    "        drop_idxs = list()\n",
    "        for line_idx in text_df['line_idx'].unique():\n",
    "            line_df = text_df.loc[text_df['line_idx'] == line_idx, :]\n",
    "\n",
    "            line_df.index.names=['original_index']\n",
    "            line_df = line_df.sort_values(by='left', \n",
    "                                          ascending=True, \n",
    "                                          ignore_index=False, \n",
    "                                          inplace=False\n",
    "                                          )\n",
    "            line_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "            drop_idxs.extend(line_df['original_index'].values[1:].tolist())\n",
    "\n",
    "            line_df = self.combine_key_value_pairs_in_words_df(line_df)\n",
    "\n",
    "            if line_idx == 35:\n",
    "                stop  = [] \n",
    "            col_phrases = self.identify_columns_from_words_df(line_df.copy())\n",
    "\n",
    "            text_df.loc[line_df.loc[0, 'original_index'], 'text'] = self.col_sep_str.join(col_phrases)\n",
    "\n",
    "        self.ocr_words_df = text_df.copy()\n",
    "        text_df = text_df.drop(drop_idxs).reset_index(drop=True)\n",
    "\n",
    "        return text_df\n",
    "\n",
    "    def combine_merge_sets_docTR(self, text_df=None):\n",
    "\n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "\n",
    "        drop_idxs = list()\n",
    "        for line_idx in text_df['line_idx'].unique():\n",
    "            line_df = text_df.loc[text_df['line_idx'] == line_idx, :]\n",
    "\n",
    "            # replace first line in merge set with merged text and position info, then store indices of \n",
    "            # remaining merge set to drop at end of combine method\n",
    "            text_df.loc[line_df.index[0], \n",
    "                        ['bottom', \n",
    "                         'left', \n",
    "                         'right', \n",
    "                         'norm_bottom',\n",
    "                         'top',\n",
    "                         'norm_top',\n",
    "                         ]] = [line_df['bottom'].max(),\n",
    "                               line_df['left'].min(),\n",
    "                               line_df['right'].max(),\n",
    "                               line_df['norm_bottom'].max(),\n",
    "                               line_df['top'].min(),\n",
    "                               line_df['norm_top'].min(),] \n",
    "\n",
    "            drop_idxs.extend(line_df.index[1:])             \n",
    "\n",
    "        text_df = text_df.drop(drop_idxs).reset_index(drop=True)\n",
    "\n",
    "    def identify_line_merge_sets_docTR(self, text_df=None):\n",
    "        \n",
    "        if text_df is None:\n",
    "            text_df = self.text_df\n",
    "        \n",
    "        same_line_overlap_thresh = 0.005\n",
    "\n",
    "        merge_sets = list()\n",
    "        for idx, line in text_df.iterrows():\n",
    "            merge_set = np.where((text_df['norm_top']    < line['norm_bottom']) &\n",
    "                                 (np.abs(text_df['norm_top'] - line['norm_bottom']) > same_line_overlap_thresh) &\n",
    "                                 (text_df['norm_top']   >= line['norm_top']   )  )[0]\n",
    "            merge_set = text_df.index[merge_set]\n",
    "            if len(merge_set) > 0:\n",
    "                merge_set = merge_set.to_list()\n",
    "                merge_set = sorted(merge_set.append(idx)) if idx not in merge_set else merge_set \n",
    "                same_merge_set   = any([True if m_set == merge_set else False for m_set in merge_sets])\n",
    "                overlapping_sets = [set_idx for set_idx, m_set in enumerate(merge_sets) if any(i for i in m_set if i in merge_set)]\n",
    "                if same_merge_set:\n",
    "                    continue\n",
    "                elif len(overlapping_sets) == 1:\n",
    "                    merge_sets[overlapping_sets[0]] = np.unique(merge_sets[overlapping_sets[0]] + merge_set).tolist() \n",
    "                elif len(overlapping_sets) > 1:\n",
    "                    raise ValueError('Have not written code to manage more than one overlapping set when combining lines')\n",
    "                else:\n",
    "                    merge_sets.append(merge_set)\n",
    "                    # print(' '.join(text_df.loc[merge_set,['text','left']].sort_values(by='left').text))\n",
    "        text_df['line_idx'] = np.full((text_df.shape[0],), -1, dtype=int)\n",
    "        for msIdx, merge_set in enumerate(merge_sets):\n",
    "            text_df.loc[merge_set, 'line_idx'] = np.repeat(msIdx, len(merge_set))\n",
    "\n",
    "        return text_df\n",
    "\n",
    "    def organize_text_lines_by_row_and_column_docTR(self, text_df=None):\n",
    "        # text_df = self.identify_line_merge_sets_docTR(text_df)\n",
    "        merge_sets, text_df = self.identify_line_merge_sets(text_df)\n",
    "        text_df = self.organize_single_lines_docTR(text_df) \n",
    "        # self.ocr_words_df = text_df.copy()\n",
    "        # text_df = self.condense_merge_sets(text_df, None, None)\n",
    "        return text_df\n",
    "\n",
    "    def add_new_data_to_text_df(self, tmp_text_df):\n",
    "        drop_idxs =list()\n",
    "        for idx, line in tmp_text_df.iterrows():\n",
    "            match_score = np.array([levenshtein.normalized_similarity(line['text'], match_line['text'])\n",
    "                                    if abs(line['norm_top'] - match_line['norm_top']) < 0.05\n",
    "                                    else 0\n",
    "                                    for rIdx, match_line in self.text_df.iterrows()])\n",
    "            if any(match_score > 0.8):\n",
    "                drop_idxs.append(idx)\n",
    "\n",
    "        tmp_text_df = tmp_text_df.drop(drop_idxs)\n",
    "\n",
    "        self.text_df = pd.concat((self.text_df, tmp_text_df), axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    def get_text_lines_from_ocr(self, pages: Optional[Iterable[int]] = None) -> pd.DataFrame:\n",
    "        text_dict = dict(text=[], source=[], page=[], top=[], bottom=[], left=[], right=[], norm_top=[], norm_bottom=[], confidence=[])\n",
    "        for page_idx, page in enumerate(self.ocr_text['pages']):\n",
    "            for block in page['blocks']:\n",
    "                for line in block['lines']:\n",
    "                    for word in line['words']:\n",
    "                        text_dict['page'       ].append(page_idx)\n",
    "                        text_dict['bottom'     ].append(page['dimensions'][0] * word['geometry'][1][1])\n",
    "                        text_dict['top'        ].append(page['dimensions'][0] * word['geometry'][0][1])\n",
    "                        text_dict['left'       ].append(page['dimensions'][1] * word['geometry'][0][0])\n",
    "                        text_dict['right'      ].append(page['dimensions'][1] * word['geometry'][1][0])\n",
    "                        text_dict['text'       ].append(word['value'].lower())\n",
    "                        text_dict['norm_top'   ].append(page_idx+word['geometry'][0][1])\n",
    "                        text_dict['norm_bottom'].append(page_idx+word['geometry'][1][1])\n",
    "                        text_dict['confidence' ].append(word['confidence'])\n",
    "                        text_dict['source'     ].append('ocr') \n",
    "        ocr_text_df = pd.DataFrame(text_dict)\n",
    "        ocr_text_df = ocr_text_df.sort_values(by=['norm_top', 'left'], ascending=True, ignore_index=True)\n",
    "\n",
    "        ocr_text_df = self.organize_text_lines_by_row_and_column_docTR(text_df=ocr_text_df)        \n",
    "        \n",
    "        if self.text_df is None:\n",
    "            self.text_df = ocr_text_df\n",
    "        elif len(ocr_text_df) > 0:\n",
    "            self.add_new_data_to_text_df(ocr_text_df)\n",
    "\n",
    "        self.text_df.sort_values(by='norm_top', inplace=True, ignore_index=True, ascending=True)\n",
    "\n",
    "    def get_text_lines_from_ocr_AFTER_TESSERACT(self, pages: Optional[Iterable[int]] = None) -> pd.DataFrame:\n",
    "        text_dict = dict(text=[], source=[], page=[], top=[], bottom=[], left=[], right=[], norm_top=[], norm_bottom=[],)\n",
    "        for page_idx, page in enumerate(extract_pages(self.ocr_filepath)):\n",
    "            pdf = pdfplumber.open(self.ocr_filepath)\n",
    "            for el_idx, element in enumerate(page):\n",
    "                if isinstance(element, LTTextContainer):\n",
    "                    did_split = self.split_text_lines_with_pdfplumber(pdf, element, page_idx, text_dict)\n",
    "                    if not did_split:\n",
    "                        top = page.height - element.y1\n",
    "                        bottom = page.height - element.y0 \n",
    "                        norm_top    = page_idx + top    / page.height\n",
    "                        norm_bottom = page_idx + bottom / page.height\n",
    "                        if np.logical_not(np.any((np.isclose(self.text_df['norm_top'   ], norm_top   , atol=1e-2)) &\n",
    "                                                 (np.isclose(self.text_df['norm_bottom'], norm_bottom, atol=1e-2))  )):\n",
    "                            text_dict['page'       ].append(page_idx)\n",
    "                            text_dict['bottom'     ].append(bottom)\n",
    "                            text_dict['top'        ].append(top)\n",
    "                            text_dict['left'       ].append(element.x0)\n",
    "                            text_dict['right'      ].append(element.x1)\n",
    "                            text_dict['text'       ].append(element.get_text().lower().replace('\\n', '').replace('|', ''))\n",
    "                            text_dict['norm_top'   ].append(norm_top)\n",
    "                            text_dict['norm_bottom'].append(norm_bottom)\n",
    "                            text_dict['source'     ].append('ocr')\n",
    "    \n",
    "                            print(f'\\nstoring text from OCR on page {page_idx}:\\n\"{element.get_text().lower().replace('\\n', '')}\"')\n",
    "\n",
    "                elif isinstance(element, LTRect):\n",
    "                    print(f'page = {page_idx}, ypos={element.y0}, type={type(element)}')\n",
    "                else:\n",
    "                    print(f'page = {page_idx}, ypos={element.y0}, type={type(element)}')\n",
    "\n",
    "        if self.text_df is None:\n",
    "            self.text_df = pd.DataFrame(text_dict)\n",
    "        elif len(text_dict['text']) > 0:\n",
    "            tmp_text_df  = pd.DataFrame(text_dict)\n",
    "            self.text_df = pd.concat((self.text_df, tmp_text_df), axis=0, ignore_index=True)\n",
    "\n",
    "        self.text_df.sort_values(by='norm_top', inplace=True, ignore_index=True, ascending=True)\n",
    "\n",
    "    def get_section_headers(self):\n",
    "        with open(self.config, 'r') as file:\n",
    "            self.config_data = yaml.safe_load(file) \n",
    "        \n",
    "        self.sections = dict()\n",
    "        for section in self.config_data['sections']:\n",
    "            self.sections[section['header']] = dict(bounds = pd.DataFrame(),\n",
    "                                                    extract = section['extract'])\n",
    "            for key in section.keys():\n",
    "                if key in ['header', 'extract']: continue\n",
    "                self.sections[section['header']][key] = section[key]\n",
    "\n",
    "    def get_section_bounds(self):\n",
    "\n",
    "        self.get_section_headers()\n",
    "\n",
    "        # detect start and end of sections\n",
    "        prev_section_info = dict(header=None, df_row=None)\n",
    "        prev_page = 0\n",
    "        for row_idx, text_data in self.text_df.iterrows():\n",
    "            detected_section = None\n",
    "            for header in self.sections.keys():\n",
    "                match = regex.search(f'{header}{{s<=3,i<=3,d<=3}}', text_data['text'], regex.BESTMATCH)    \n",
    "                if match is not None:\n",
    "                    detected_section = header\n",
    "                    break\n",
    "\n",
    "            if text_data['page'] > prev_page or detected_section is not None:\n",
    "                if prev_section_info['header'] is not None:   # store end of sections\n",
    "                    prev_df_idx = self.sections[prev_section_info['header']]['bounds'].index[-1] \n",
    "                    prev_bottom = prev_page+0.9999 if (text_data['page'] > prev_page) else text_data['norm_top'] \n",
    "                    self.sections[prev_section_info['header']]['bounds'].loc[prev_df_idx, ['bottom',]] = [prev_bottom]\n",
    "                # store start of sections\n",
    "                top = text_data['page'] if (text_data['page'] > prev_page) else text_data['norm_top']\n",
    "                tmp_df = pd.DataFrame(data    = zip([top], [np.nan], [False]),\n",
    "                                      columns = ['top', 'bottom', 'data_extracted'],)\n",
    "                header = detected_section\n",
    "                self.sections[header]['bounds'] = pd.concat((self.sections[header]['bounds'], tmp_df), ignore_index=True)\n",
    "                prev_section_info['header'] = header\n",
    "                if text_data['page'] > prev_page:\n",
    "                    prev_page+=1\n",
    "            prev_section_info['df_row'] = row_idx\n",
    "        \n",
    "        # store end of final section\n",
    "        prev_bottom = self.text_df.loc[prev_section_info['df_row'], 'norm_bottom']\n",
    "        prev_df_idx = self.sections[prev_section_info['header']]['bounds'].index[-1] \n",
    "        self.sections[prev_section_info['header']]['bounds'].loc[prev_df_idx, ['bottom',]] = [np.ceil(prev_bottom),]\n",
    "    \n",
    "    def extract_table(self, subsection_bounds):  \n",
    "        page_num = int(np.floor(subsection_bounds['top'])) \n",
    "        pdf = pdfplumber.open(self.orig_filepath)\n",
    "        table_page = pdf.pages[page_num]\n",
    "        table_crop = table_page.within_bbox((               0, (subsection_bounds['top'   ]-page_num)*table_page.height, \n",
    "                                             table_page.width, (subsection_bounds['bottom']-page_num)*table_page.height))\n",
    "        table = table_crop.extract_table()\n",
    "        # print((subsection_bounds['top'   ]-page_num)*table_page.height, (subsection_bounds['bottom']-page_num)*table_page.height, table)\n",
    "        return table\n",
    "    \n",
    "    def extract_text_lines_from_original(self, subsection_bounds):\n",
    "        page_num = int(np.floor(subsection_bounds['top'])) \n",
    "        pdf = pdfplumber.open(self.orig_filepath)\n",
    "        page = pdf.pages[page_num]\n",
    "        page_crop = page.within_bbox((         0, (subsection_bounds['top'   ]-page_num)*page.height, \n",
    "                                      page.width, (subsection_bounds['bottom']-page_num)*page.height))  \n",
    "\n",
    "        text = page_crop.extract_text_lines()\n",
    "\n",
    "        for line in text:\n",
    "            line['norm_top']    = page_num + (line['top']    / page.height)\n",
    "            line['norm_bottom'] = page_num + (line['bottom'] / page.height)\n",
    "            line['page']        = page_num\n",
    "            line['bottom']      = page.height - line['bottom']\n",
    "            line['top']         = page.height - line['top']\n",
    "            line['text']        = line['text'].lower().replace('|', '')\n",
    "\n",
    "        return text  \n",
    "\n",
    "    def table_to_df(self, \n",
    "                    table: List[List[str]], \n",
    "                    extract_params: str | Dict | List[Dict],\n",
    "                    ) -> pd.DataFrame:\n",
    "        if type(extract_params) == str and 'col' in extract_params.lower():\n",
    "            info_keys = list()\n",
    "            data = list()\n",
    "            for row in table:\n",
    "                iKey = row.pop(0)\n",
    "                info_keys.append(iKey.replace('\\n', ' '))\n",
    "                data.append(row)\n",
    "            df = pd.DataFrame(data=np.array(data).transpose(), columns=info_keys)\n",
    "\n",
    "        elif type(extract_params) == str and 'row' in extract_params.lower():\n",
    "            info_keys = table[0]\n",
    "            data = table[1:]\n",
    "            df = pd.DataFrame(data=data, columns=info_keys)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('There is no method implemented for converting data with this extraction method to a DataFrame')\n",
    "        \n",
    "        return df \n",
    " \n",
    "    def extract_table_data(self):\n",
    "        for section_header, section_dict in self.sections.items():\n",
    "            for sub_idx, subsection_bounds in section_dict['bounds'].iterrows():\n",
    "                if subsection_bounds['data_extracted']:\n",
    "                    continue\n",
    "\n",
    "                table = self.extract_table(subsection_bounds)\n",
    "                if table is not None:\n",
    "                    data_df = self.table_to_df(table, section_dict['extract'])\n",
    "                    data_df = data_df.loc[:, [col for col in data_df.columns if col.lower() != section_header]]\n",
    "                    if 'data' in section_dict.keys(): \n",
    "                        section_dict['data'] = pd.concat((section_dict['data'], data_df),\n",
    "                                                         axis=0,\n",
    "                                                         ignore_index=True)\n",
    "                    else:\n",
    "                        section_dict['data'] = data_df\n",
    "                    \n",
    "                    section_dict['bounds'].loc[sub_idx, 'data_extracted'] = True\n",
    "\n",
    "    def align_data_to_existing_df(self,\n",
    "                                  section_dict: Dict,\n",
    "                                  subsect_df: pd.DataFrame,\n",
    "                                 ):\n",
    "        if type(section_dict['extract']) == str and 'col' in section_dict['extract'].lower():\n",
    "            info_keys_to_match = list(section_dict['data'].columns)\n",
    "            original_info_keys = list(section_dict['data'].columns)\n",
    "            info_keys = list()\n",
    "            data      = list()\n",
    "            for text in subsect_df['text']:\n",
    "                row = text.split(self.col_sep_str)\n",
    "                iKey = row.pop(0)\n",
    "                iKey = iKey.replace('\\n', ' ')\n",
    "                match_score = np.array([levenshtein.normalized_similarity(iKey.lower(), matchKey.lower()) for matchKey in info_keys_to_match])\n",
    "                sorted_match_score, sorted_info_keys = zip(*sorted(zip(match_score, info_keys_to_match), reverse=True))\n",
    "                if iKey == 'equipment tip':\n",
    "                    stop = []\n",
    "                try:\n",
    "                    top_key_matches = sorted_info_keys[:3]\n",
    "                    \n",
    "                    #correct any instances in which OCR dropped the last word (probably b/c it was on a second line), causing poor matching\n",
    "                    if (sorted_match_score[0] < 0.9 \n",
    "                        and len(iKey.split(' ')) == len(top_key_matches[0].split(' ')) \n",
    "                        and any([len(iKey.split(' ')) < len(key.split(' ')) for key in top_key_matches])):\n",
    "                        match_score = np.array([levenshtein.normalized_similarity(iKey.lower(), ' '.join(matchKey.lower().split(' ')[:-1])) for matchKey in info_keys_to_match])\n",
    "            \n",
    "                    matched_key = info_keys_to_match.pop(np.where(match_score == match_score.max())[0][0])\n",
    "                    info_keys.append(matched_key)\n",
    "                    data.append(row)\n",
    "                    # print(iKey, matched_key)\n",
    "                except:\n",
    "                    print(f'\\nNo matched key: iKey={iKey}, text={text}')\n",
    "            \n",
    "            # Add dummy data for unmatched keys\n",
    "            expected_num_items = pd.Series([len(d) for d in data]).mode()[0]\n",
    "            for key in info_keys_to_match:\n",
    "                info_keys.append(key)\n",
    "                data.append(['data_not_found' for k in range(expected_num_items)])\n",
    "\n",
    "            # expected_num_items = pd.Series([len(d) for d in data]).mode()[0]\n",
    "            wrong_count_info = [(idx, len(d)) for idx, d in enumerate(data) if len(d) != expected_num_items]\n",
    "            for idx, num_items in wrong_count_info:\n",
    "                data[idx] = ['wrong_num_columns' for k in range(expected_num_items)]\n",
    "                print(f'\\n\"{info_keys[idx]}\" contained the wrong number of columns in the line.')\n",
    "\n",
    "            correct_order = [np.where(np.array(original_info_keys) == key)[0][0] for key in info_keys]\n",
    "            _, info_keys = zip(*sorted(zip(correct_order, info_keys)))\n",
    "            _, data      = zip(*sorted(zip(correct_order, data))) \n",
    "\n",
    "            data_df = pd.DataFrame(data=np.array(data).transpose(), columns=info_keys)\n",
    "            section_dict['data'] = pd.concat((section_dict['data'], data_df),\n",
    "                                              axis=0,\n",
    "                                              ignore_index=True)\n",
    "        return\n",
    "\n",
    "    def get_multilevel_key_value_pairs(self, section_dict, items):\n",
    "        filling_subheader = False\n",
    "        for item in items: \n",
    "            split_item = item.split(self.key_val_sep)\n",
    "            if len(split_item) == 2:\n",
    "                key, value = split_item\n",
    "                if len(value) == 0:\n",
    "                    value = None\n",
    "                else:\n",
    "                    value = value[1:]  if value[0]  == ' '  else value\n",
    "                    value = value[:-1] if value[-1] == '\\n' else value\n",
    "                    \n",
    "                if filling_subheader:\n",
    "                    section_dict['data'][stored_key][key] = value\n",
    "                    # print(f'{stored_key} - {key}{self.key_val_sep} {value}')\n",
    "                else:\n",
    "                    section_dict['data'][key] = value     \n",
    "                    # print(f'{key}{self.key_val_sep} {value}')\n",
    "            elif len(split_item) == 1:\n",
    "                stored_key = split_item[0]\n",
    "                section_dict['data'][stored_key] = dict()\n",
    "                filling_subheader = True\n",
    "\n",
    "    def extract_key_value_pairs(self, \n",
    "                                section_dict: Dict, \n",
    "                                subsect_df: pd.DataFrame, \n",
    "                                ) -> Dict:\n",
    "        section_dict['data'] = dict()\n",
    "        for l_idx, line in subsect_df.iterrows(): \n",
    "            items = line['text'].split(self.col_sep_str)\n",
    "            key_value_pairs = [item for item in items if len(item.split(self.key_val_sep)) == 2]\n",
    "            if len(key_value_pairs) == len(items):\n",
    "                for item in key_value_pairs:\n",
    "                    key, value = item.split(self.key_val_sep)\n",
    "                    if len(value) == 0:\n",
    "                        value = None\n",
    "                    else:\n",
    "                        value = value[1:]  if value[0]  == ' '  else value\n",
    "                        value = value[:-1] if value[-1] == '\\n' else value\n",
    "                    section_dict['data'][key] = value     \n",
    "                    # print(f'{key}{self.key_val_sep} {value}') \n",
    "            else:\n",
    "                if line['source'] == 'original':\n",
    "                    pdf = pdfplumber.open(self.orig_filepath)\n",
    "                elif line['source'] == 'ocr':\n",
    "                    pdf = pdfplumber.open(self.ocr_filepath)\n",
    "                self.get_multilevel_key_value_pairs(section_dict, items)\n",
    "\n",
    "    def extract_text_data(self):\n",
    "        for section_header, section_dict in self.sections.items():\n",
    "            for sub_idx, subsection_bounds in section_dict['bounds'].iterrows():\n",
    "                if subsection_bounds['data_extracted']:\n",
    "                    continue\n",
    "\n",
    "                mask = (self.text_df['norm_top'   ] > subsection_bounds['top'   ]) & \\\n",
    "                       (self.text_df['norm_bottom'] < subsection_bounds['bottom']) \n",
    "                subsect_df = self.text_df.loc[mask, :]\n",
    "                \n",
    "                if 'data' in section_dict.keys() and type(section_dict['data']) == pd.DataFrame:\n",
    "                    self.align_data_to_existing_df(section_dict, subsect_df)\n",
    "                else:\n",
    "                    self.extract_key_value_pairs(section_dict, subsect_df)\n",
    "                \n",
    "                section_dict['bounds'].loc[sub_idx, 'data_extracted'] = True\n",
    "\n",
    "    def print_text(self):\n",
    "        max_x = 0\n",
    "        for page_text in self.text_containers:\n",
    "            for text_container in page_text:\n",
    "                if text_container.x1 > max_x:\n",
    "                    max_x = text_container.x1\n",
    "        for page_num, page_text in enumerate(self.text_containers):\n",
    "            print('###############################################')\n",
    "            print(f'Page {page_num}')\n",
    "            print('###############################################') \n",
    "            for text_container in page_text:  \n",
    "                text = text_container.get_text()\n",
    "                # print(f'{text_container.y0} to {text_container.y1}', text)\n",
    "                print(text_container.y1, text_container.y0, text)\n",
    "\n",
    "    def convert_pdf_page_to_image(self, pymupdf_doc, image_path, idx, zoom=1):\n",
    "        mat = pymupdf.Matrix(zoom, zoom)\n",
    "        page = pymupdf_doc.load_page(idx)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        pix.save(image_path)\n",
    "\n",
    "    def write_ocr_text_to_pdfa(self, pdf_outpath, hocr_path, image_path):\n",
    "        hocr = HocrTransform(hocr_filename=hocr_path,\n",
    "                            dpi=100,)\n",
    "        hocr.to_pdf(\n",
    "                    out_filename=pdf_outpath,\n",
    "                    image_filename=image_path,\n",
    "                    )\n",
    "        \n",
    "    def write_hocr_xml_file(self, hocr_path, page_xml):\n",
    "        with open(hocr_path, 'w') as f:\n",
    "            f.write(page_xml[0].decode())\n",
    "\n",
    "    def create_pdfa_with_ocr(self, output_dir, output_stem, ocr_xml, pymupdf_doc):\n",
    "        merger = PdfMerger()\n",
    "        with TemporaryDirectory(dir= Path(os.getcwd())) as tmpdir:\n",
    "            tmppath = Path(tmpdir)\n",
    "            for idx, page_xml in enumerate(ocr_xml): \n",
    "                hocr_path   = tmppath / f'{output_stem}_hocr_page{idx}.xml'\n",
    "                image_path  = tmppath / f'{output_stem}_image_page{idx}.png'\n",
    "                pdf_outpath = tmppath / f'{output_stem}_docTR_page{idx}.pdf'\n",
    "                self.write_hocr_xml_file(hocr_path, page_xml)\n",
    "                self.convert_pdf_page_to_image(pymupdf_doc, image_path, idx, zoom=4) \n",
    "                self.write_ocr_text_to_pdfa(pdf_outpath, hocr_path, image_path)\n",
    "                merger.append(pdf_outpath)\n",
    "            merger.write(output_dir / f'{output_stem}_docTR.pdf' )\n",
    "            merger.close()\n",
    "\n",
    "    def run_ocr(self, ocr_predictor):\n",
    "        output_base_path = self.orig_filepath.parent / self.orig_filepath.stem\n",
    "        doc_pages = DocumentFile.from_pdf(self.orig_filepath)\n",
    "        pymupdf_doc = pymupdf.open(self.orig_filepath)\n",
    "        ocr_text = ocr_predictor(doc_pages) \n",
    "        self.ocr_text = ocr_text.export()\n",
    "        ocr_xml = ocr_text.export_as_xml()\n",
    "        self.create_pdfa_with_ocr(output_dir  = self.orig_filepath.parent, \n",
    "                                  output_stem = self.orig_filepath.stem, \n",
    "                                  ocr_xml = ocr_xml, \n",
    "                                  pymupdf_doc = pymupdf_doc)   \n",
    "\n",
    "class ATC_amendment(pdf_data):\n",
    "    def __init__(self, \n",
    "                 orig_filepath: str | Path, \n",
    "                 ocr_filepath:  str | Path, \n",
    "                 config:        str | Path,\n",
    "                 key_val_sep:   str = ':' ,) -> None:        \n",
    "        super().__init__(orig_filepath, ocr_filepath, config, key_val_sep)\n",
    "\n",
    "    def extract_line_config_data_from_cells_containing_all_key_value_pairs(self, \n",
    "                                                                           equipment_df, \n",
    "                                                                           configuration_col,\n",
    "                                                                           data_dict, \n",
    "                                                                           data_keys,\n",
    "                                                                           storage_key):\n",
    "        line_config_df = equipment_df.loc[:, configuration_col]\n",
    "        for equipIdx, cell in line_config_df.items():\n",
    "            for dKey in data_keys:\n",
    "                num_match = len(regex.findall(f'{dKey}{{e<=1}}', cell))\n",
    "                \n",
    "                if num_match > 0:\n",
    "                    # move thru each match (there may be multiple line configs in single cell)\n",
    "                    prev_search_end = 0\n",
    "                    for idx in range(num_match):\n",
    "                        match = regex.search(f'{dKey}{{e<=1}}', cell[prev_search_end:], pos=idx)\n",
    "                        key_span = (match.span()[0] + prev_search_end, match.span()[1] + prev_search_end)\n",
    "                        \n",
    "                        # find the next key match in the string to know where the value for this key ends\n",
    "                        possible_next_keys_pos = list()\n",
    "                        for next_dKey in data_keys:\n",
    "                            next_key_match = regex.search(f'{next_dKey}{{e<=1}}', cell[key_span[1]:], pos=0) \n",
    "                            if next_key_match is not None:\n",
    "                                possible_next_keys_pos.append(next_key_match.span()[0])\n",
    "                        # extract the data token from the string\n",
    "                        if len(possible_next_keys_pos) == 0:\n",
    "                            data_token = cell[key_span[0] : ]\n",
    "                        else:\n",
    "                            token_end = key_span[1] + min(possible_next_keys_pos) \n",
    "                            data_token = cell[key_span[0] : token_end]\n",
    "                            prev_search_end = token_end\n",
    "                        data_token = data_token[ :-1] if data_token[-1] == '\\n' else data_token\n",
    "                        data_token = data_token.replace('\\n', '')\n",
    "                        # print(data_token)\n",
    "                        val = data_token.split(self.key_val_sep)[1]\n",
    "                        val = val[1:] if val[0] == ' ' else val\n",
    "\n",
    "                        data_dict[str(idx)][dKey][equipIdx] = val\n",
    "\n",
    "        for line_num, line_data in data_dict.items():\n",
    "            if any([True if any([True if val is not None else False for val in data_list]) else False \n",
    "                    for tmp_key, data_list in line_data.items()]):\n",
    "                for dKey, values in line_data.items():\n",
    "                    key = f'{storage_key}_{line_num}_{dKey}'\n",
    "                    equipment_df[key] = values \n",
    "        \n",
    "    def extract_line_config_data_from_separated_cells(self, \n",
    "                                                      equipment_df, \n",
    "                                                      configuration_cols,\n",
    "                                                      data_dict, \n",
    "                                                      data_keys,\n",
    "                                                      storage_key): \n",
    "        line_config_df = equipment_df.loc[:, configuration_cols]\n",
    "        for equipIdx, row in line_config_df.iterrows():\n",
    "            if levenshtein.normalized_similarity(row['Line Configuration'].lower(), 'n/a') < 0.3:\n",
    "                line_config = regex.split(r'\\n|;| - |-', row['Line Configuration']) \n",
    "                line_config  = [token.strip() for token in line_config if token != '']\n",
    "\n",
    "                # FIXME May need to fix this fix if it doesn't apply to other versions of old exhibit. \n",
    "                combine_idxs = [(idx, idx+1) \n",
    "                                for idx, (token, next_token) \n",
    "                                in enumerate(zip(line_config[:-1], line_config[1:])) \n",
    "                                if token[-1]=='\"' and next_token[0]=='(']  \n",
    "                for cIdxs in combine_idxs:\n",
    "                    line_config[cIdxs[0]] = ' '.join(line_config[cIdxs[0]:cIdxs[1]+1])\n",
    "                    del line_config[cIdxs[1]]\n",
    "                # fix end \n",
    "                \n",
    "                nKeys = len(data_dict['0'].keys())\n",
    "                if not len(line_config) % nKeys == 0:\n",
    "                    raise ValueError(\"The number of tokens in line_config is not a multiple of the expected number of keys for storing data\")\n",
    "                \n",
    "                equip_idx_list = [int(idx/nKeys) for idx in range(len(line_config))]\n",
    "                for dKey, token, equipIdx in zip(data_dict['0'].keys(), line_config, equip_idx_list):\n",
    "                    data_dict['0'][dKey][equipIdx] = token    \n",
    "\n",
    "            else:\n",
    "                for dKey, col in zip(data_dict['0'].keys(), line_config_df.columns):\n",
    "                    data_dict['0'][dKey][equipIdx] = row[col]\n",
    "\n",
    "        for line_num, line_data in data_dict.items():\n",
    "            if any([True if any([True if val is not None else False for val in data_list]) else False \n",
    "                    for tmp_key, data_list in line_data.items()]):\n",
    "                storage_key_list = []\n",
    "                conduit_idxs = [idx for idx, line_type in enumerate(line_data['Type']) \n",
    "                                if line_type is not None \n",
    "                                and levenshtein.normalized_similarity(line_type.lower(), 'conduit') > 0.5]\n",
    "                standard_idxs = np.setdiff1d(np.arange(len(line_data['Type']), dtype=int), conduit_idxs)\n",
    "                # store standard line data \n",
    "                for dKey, values in line_data.items():\n",
    "                    values = [val if idx in standard_idxs else None for idx, val in enumerate(values)]\n",
    "                    key = f'{storage_key}_{line_num}_{dKey}'\n",
    "                    equipment_df[key] = values       \n",
    "                # store conduit data \n",
    "                tmp_storage_key = 'conduit_config'\n",
    "                for dKey, values in line_data.items():\n",
    "                    values = [val if idx in conduit_idxs else None for idx, val in enumerate(values)]\n",
    "                    key = f'{tmp_storage_key}_{line_num}_{dKey}'\n",
    "                    \n",
    "                    # specific fixes for differences in the way that old_exhibits store conduit config data\n",
    "                    # ------------------------\n",
    "                    if dKey == 'Type':\n",
    "                        key = key.replace(dKey, 'containing')\n",
    "                        values = [val if val is None else '-;' for val in values]\n",
    "                    elif dKey == 'Diameter':\n",
    "                        key = key.replace(dKey, 'Type') \n",
    "                    # ------------------------\n",
    "                    \n",
    "                    equipment_df[key] = values   \n",
    "\n",
    "    def align_line_configuration_data(self, group_key, max_line_types=2, type_key=None):\n",
    "        \n",
    "        pdf_data_keys    = self.sections['equipment specifications'][f'{group_key} keys']\n",
    "        equipment_df = self.sections['equipment specifications']['data']\n",
    "        \n",
    "        if 'separated' in group_key:\n",
    "            storage_key = f\"{type_key.split(' ')[0]}_config\" \n",
    "            storage_data_keys = [self.sections['equipment specifications'][config_group_key] \n",
    "                                 for config_group_key in self.sections['equipment specifications'].keys() \n",
    "                                 if type_key in config_group_key][0]\n",
    "            configuration_cols = list()\n",
    "            for dKey in pdf_data_keys:\n",
    "                config_col_match_scores = [(col, levenshtein.normalized_similarity(col, dKey)) for col in equipment_df.columns]\n",
    "                scores = np.array([score for _, score in config_col_match_scores])\n",
    "                config_col = config_col_match_scores[np.argmax(scores)][0]    \n",
    "                configuration_cols.append(config_col)\n",
    "        else:\n",
    "            storage_key = f\"{group_key.split(' ')[0]}_config\"\n",
    "            storage_data_keys = pdf_data_keys\n",
    "            configuration_cols = [col for col in equipment_df if group_key in col.lower()]\n",
    "            \n",
    "        data_dict = dict()\n",
    "        for idx in range(max_line_types): \n",
    "            data_dict[str(idx)] = dict()\n",
    "            for dKey in storage_data_keys:\n",
    "                data_dict[str(idx)][dKey] = [None for k in range(equipment_df.shape[0])]\n",
    "\n",
    "        if 'separated' in group_key:\n",
    "            self.extract_line_config_data_from_separated_cells(equipment_df,\n",
    "                                                               configuration_cols,\n",
    "                                                               data_dict,\n",
    "                                                               pdf_data_keys,\n",
    "                                                               storage_key)\n",
    "        else:\n",
    "            self.extract_line_config_data_from_cells_containing_all_key_value_pairs(equipment_df, \n",
    "                                                                                    configuration_cols[0],\n",
    "                                                                                    data_dict, \n",
    "                                                                                    pdf_data_keys,\n",
    "                                                                                    storage_key,)\n",
    "           \n",
    "    def get_exhibit_name(self):\n",
    "        for text_container in self.text_containers[0]:\n",
    "            text = text_container.get_text()\n",
    "            if 'exhibit' in text.lower():\n",
    "                pattern = regex.compile(r'^\\s+')\n",
    "                exhibit = pattern.sub('', text.lower().replace('exhibit', '').replace('\\n', ''))\n",
    "                self.exhibit = exhibit\n",
    "\n",
    "class version_comparison():\n",
    "    def __init__(self, \n",
    "                 new_version: pdf_data | ATC_amendment, \n",
    "                 old_version: pdf_data | ATC_amendment, \n",
    "                 ) -> None:        \n",
    "        self.new_version = new_version\n",
    "        self.old_version = old_version\n",
    "        self.unique_key_value_pairs = dict(section=[], key0=[], key1=[], key2=[], value=[], version=[])\n",
    "        self.clear_errors           = dict(section=[], key0=[], key1=[], key2=[], value_new=[], value_old=[],)\n",
    "        self.possible_errors        = dict(section=[], key0=[], key1=[], key2=[], value_new=[], value_old=[],)\n",
    "        self.identical_data         = dict(section=[], key0=[], key1=[], key2=[], value=[],)\n",
    "\n",
    "    def find_matching_keys(self, key, keys_to_match):\n",
    "        match_score = np.array([levenshtein.normalized_similarity(key.lower(), matchKey.lower()) for matchKey in keys_to_match])\n",
    "        if all(match_score < 0.8):\n",
    "            return None\n",
    "        \n",
    "        sorted_match_score, sorted_info_keys = zip(*sorted(zip(match_score, keys_to_match), reverse=True))\n",
    "        top_key_matches = sorted_info_keys[:3]\n",
    "\n",
    "        #correct any instances in which OCR dropped the last word (probably b/c it was on a second line), causing poor matching\n",
    "        if (sorted_match_score[0] < 0.9 \n",
    "            and len(key.split(' ')) == len(top_key_matches[0].split(' ')) \n",
    "            and any([len(key.split(' ')) < len(topKey.split(' ')) for topKey in top_key_matches])):\n",
    "            match_score = np.array([levenshtein.normalized_similarity(key.lower(), ' '.join(matchKey.lower().split(' ')[:-1])) for matchKey in top_key_matches])\n",
    "\n",
    "        if match_score.max() < 0.8:\n",
    "            return None\n",
    "        else:\n",
    "            match_idx = np.where(match_score == match_score.max())[0][0]\n",
    "            matched_key = [mKey for idx, mKey in enumerate(keys_to_match) if idx == match_idx][0]\n",
    "            return matched_key\n",
    "\n",
    "    def store_identical_data(self, storage_keys, item):\n",
    "        for store_key, store_info in zip([      'section',          'key0',          'key1',          'key2', 'value',],\n",
    "                                         [storage_keys[0], storage_keys[1], storage_keys[2], storage_keys[3],   item,]):\n",
    "            self.identical_data[store_key].append(store_info)         \n",
    "\n",
    "    def store_unique_data(self, storage_keys, item, version):\n",
    "        if isinstance(item, (str, int, float)):\n",
    "            store_item = item\n",
    "        else:\n",
    "            store_item = None    \n",
    "        for store_key, store_info in zip([      'section',          'key0',          'key1',          'key2',    'value', 'version',],\n",
    "                                         [storage_keys[0], storage_keys[1], storage_keys[2], storage_keys[3], store_item,   version,]):\n",
    "            self.unique_key_value_pairs[store_key].append(store_info) \n",
    "\n",
    "    def store_possible_errors(self, storage_keys, item0, item1,):\n",
    "        for store_key, store_info in zip([      'section',          'key0',          'key1',          'key2', 'value_new', 'value_old'],\n",
    "                                         [storage_keys[0], storage_keys[1], storage_keys[2], storage_keys[3],       item0,       item1]):\n",
    "            self.possible_errors[store_key].append(store_info)      \n",
    "\n",
    "    def store_clear_errors(self, storage_keys, item0, item1,):\n",
    "        for store_key, store_info in zip([      'section',          'key0',          'key1',          'key2', 'value_new', 'value_old'],\n",
    "                                         [storage_keys[0], storage_keys[1], storage_keys[2], storage_keys[3],       item0,       item1]):\n",
    "            self.clear_errors[store_key].append(store_info) \n",
    "\n",
    "    def check_known_substitutions(self, item0, item1, sim_score):\n",
    "        sim_scores = [sim_score]\n",
    "        sim_scores.append(levenshtein.normalized_similarity(item0.replace('i','/'), \n",
    "                                                            item1.replace('i','/')))\n",
    "        sim_scores.append(levenshtein.normalized_similarity(item0.replace('\\n',' ').replace('/','-'), \n",
    "                                                            item1.replace('\\n',' ').replace('/','-')))\n",
    "        sim_scores.append(levenshtein.normalized_similarity(item0.replace('\\n','').replace('/','-'), \n",
    "                                                            item1.replace('\\n','').replace('/','-')))\n",
    "        sim_scores.append(levenshtein.normalized_similarity(item0.replace('/','-'), \n",
    "                                                            item1.replace('/','-')))\n",
    "        sim_scores.append(levenshtein.normalized_similarity(item0.replace(' x ','x').replace(' x','x').replace('x ','x'),\n",
    "                                                            item1.replace(' x ','x').replace(' x','x').replace('x ','x')))\n",
    "        return max(sim_scores)\n",
    "\n",
    "    # def check_known_substitutions(self, item0, item1):\n",
    "    #     matched_with_known_sub = False\n",
    "    #     if item0.replace('i','/') == item1.replace('i','/'):\n",
    "    #         matched_with_known_sub = True\n",
    "    #     if item0.replace('\\n',' ') == item1.replace('\\n',' '):\n",
    "    #         matched_with_known_sub = True\n",
    "    #     if item0.replace(' x ','x').replace(' x','x').replace('x ','x') == item1.replace(' x ','x').replace(' x','x').replace('x ','x'):\n",
    "    #         matched_with_known_sub = True        \n",
    "        \n",
    "    #     return matched_with_known_sub\n",
    "\n",
    "    def compare_values(self, item0, item1, storage_keys,):\n",
    "        if item0 is None and item0 is None:\n",
    "            return\n",
    "        elif item0 is None:\n",
    "            self.store_unique_data(storage_keys, item1, version='old')\n",
    "            return\n",
    "        elif item1 is None:\n",
    "            self.store_unique_data(storage_keys, item0, version='new')\n",
    "            return\n",
    "        \n",
    "        item0, item1 = item0.lower(), item1.lower()\n",
    "        sim_score = levenshtein.normalized_similarity(item0, item1) \n",
    "        sim_score = self.check_known_substitutions(item0, item1, sim_score)\n",
    "        if sim_score == 1:\n",
    "            self.store_identical_data(storage_keys, item0)\n",
    "        else:\n",
    "            try: \n",
    "                # check if data can be converted to float, and check if identical then\n",
    "                if float(item0) == float(item1):\n",
    "                    self.store_identical_data(storage_keys, str(float(item0)))\n",
    "            except:\n",
    "                # check common substitutions\n",
    "                # matched_with_known_sub = self.check_known_substitutions(item0, item1)\n",
    "                # if matched_with_known_sub:\n",
    "                    # self.store_identical_data(storage_keys, item0)\n",
    "                # check if there is a single character substitution\n",
    "                if hamming.distance == 1 and len(item0) == len(item1):\n",
    "                    self.store_possible_errors(storage_keys, item0, item1)\n",
    "                # check if longer strings are similar\n",
    "                elif sim_score >= 0.75:\n",
    "                    self.store_possible_errors(storage_keys, item0, item1)\n",
    "                else:\n",
    "                    self.store_clear_errors(storage_keys, item0, item1)   \n",
    "\n",
    "    def compare_data_in_dfs(self, new_data, old_data, storage_keys):\n",
    "        next_storage_key_idx = [idx for idx, val in enumerate(storage_keys) if val is None][0]\n",
    "        \n",
    "        # find all matching column headers\n",
    "        old_data_columns = []\n",
    "        new_data_columns = []\n",
    "        for col in new_data.columns:\n",
    "            possible_match_columns = [col for col in old_data.columns if col not in old_data_columns]\n",
    "            matched_key = self.find_matching_keys(col, possible_match_columns)\n",
    "            if matched_key is not None:\n",
    "                old_data_columns.append(matched_key)\n",
    "                new_data_columns.append(col)\n",
    "\n",
    "        matched_new_data = new_data.loc[:, new_data_columns]\n",
    "        matched_old_data = old_data.loc[:, old_data_columns]\n",
    "\n",
    "        # then go thru rows, identify matching data\n",
    "        for rIdx_new, row_data_new in matched_new_data.iterrows():\n",
    "            for rIdx_old, row_data_old in matched_old_data.iterrows():\n",
    "                equip_storage_keys = storage_keys.copy()\n",
    "                all_fields_similarity = list()\n",
    "                for val_new, val_old in zip(row_data_new.values, row_data_old.values):\n",
    "                    if val_new is None or val_old is None:\n",
    "                        match_score = 1.0 if val_new is None and val_old is None else 0.0                    \n",
    "                    else: \n",
    "                        match_score = levenshtein.normalized_similarity(val_new.lower(), val_old.lower()) \n",
    "                    all_fields_similarity.append(match_score)        \n",
    "                \n",
    "                # check if at least 75% of entries have high similarity scores --> if so, compare values for the matched rows\n",
    "                if sum(np.array(all_fields_similarity) > 0.8) >= 0.75*len(all_fields_similarity):\n",
    "                    equip_storage_keys[next_storage_key_idx] = f'new_row_idx_{rIdx_new}-old_row_idx{rIdx_old}'\n",
    "                    for col_key, new_item, old_item in zip(row_data_new.index, row_data_new.values, row_data_old.values):\n",
    "                        equip_storage_keys[next_storage_key_idx+1] = col_key\n",
    "                        self.compare_values(new_item, old_item, equip_storage_keys) \n",
    "     \n",
    "            # if there is no matching data (sim_score < 0.8 for most entries), this is a new piece of equipment --> store in unique data\n",
    "            equip_storage_keys[next_storage_key_idx] = f'row_idx_{rIdx_new}'\n",
    "            self.store_unique_data(equip_storage_keys, item=None, version='new')\n",
    "\n",
    "    \n",
    "    def compare_data_in_dicts(self, new_data, old_data, storage_keys):\n",
    "        next_storage_key_idx = [idx for idx, val in enumerate(storage_keys) if val is None][0]\n",
    "        for key in new_data.keys():\n",
    "            storage_keys[next_storage_key_idx] = key           \n",
    "            new_item = new_data[key]\n",
    "            matched_key = self.find_matching_keys(key, old_data.keys())\n",
    "            if matched_key is None:\n",
    "                self.store_unique_data(storage_keys, item=new_item, version='new')\n",
    "                continue   \n",
    "\n",
    "            old_item = old_data[matched_key]\n",
    "        if isinstance(old_item, dict) and isinstance(new_item, dict):\n",
    "            self.compare_data_in_dicts(new_item, old_item, storage_keys)\n",
    "        elif isinstance(old_item, pd.DataFrame) and isinstance(new_item, pd.DataFrame):\n",
    "            self.compare_data_in_dfs(new_data, old_data, storage_keys)\n",
    "        else:\n",
    "            self.compare_values(new_item, old_item, storage_keys)        \n",
    "\n",
    "    def compare_data(self,):\n",
    "        self.section_comps_completed = []\n",
    "        for section_key in self.new_version.sections.keys(): \n",
    "            storage_keys = [section_key, None, None, None]\n",
    "            matched_key = self.find_matching_keys(section_key, self.old_version.sections.keys())\n",
    "            if matched_key is None:\n",
    "                print(f'The old version of the document does not contain a section titled \"{section_key}\"')\n",
    "                self.store_unique_data(storage_keys, item=None, version='new')\n",
    "                continue\n",
    "            \n",
    "            old_data = self.old_version.sections[matched_key]['data']\n",
    "            new_data = self.new_version.sections[section_key]['data'] \n",
    "            if isinstance(old_data, dict) and isinstance(new_data, dict):\n",
    "                self.compare_data_in_dicts(new_data, old_data, storage_keys)\n",
    "            elif isinstance(old_data, pd.DataFrame) and isinstance(new_data, pd.DataFrame):\n",
    "                self.compare_data_in_dfs(new_data, old_data, storage_keys)\n",
    "            elif type(old_data) != type(new_data):\n",
    "                raise ValueError(f'Data type mismatch: old_data is a {type(old_data)}, new_data is a {type(new_data)}')\n",
    "            else: \n",
    "                raise ValueError(f'Methods for comparing data of type {type(old_data)} have not been implemented')\n",
    "            self.section_comps_completed.append(section_key)\n",
    "        \n",
    "        self.identical_data = pd.DataFrame.from_dict(self.identical_data)\n",
    "        self.unique_key_value_pairs = pd.DataFrame.from_dict(self.unique_key_value_pairs)\n",
    "        self.possible_errors = pd.DataFrame.from_dict(self.possible_errors)\n",
    "        self.clear_errors = pd.DataFrame.from_dict(self.clear_errors)\n",
    "    \n",
    "\n",
    "    def add_annotation_and_save_image(self, new_doc, old_doc, image_path, page_idx, rect, fill_color=(0,0,0), zoom=1):\n",
    "        corrections_doc = pymupdf.open()\n",
    "        width, height = pymupdf.paper_size(\"a4\")  # A4 portrait output page format\n",
    "        r = pymupdf.Rect(0, 0, width, height)\n",
    "\n",
    "        # define the 2 rectangles per page\n",
    "        r1 = r / 2  # top left rect\n",
    "        r2 = r1 + (r1.width, 0, r1.width, 0)  # top right\n",
    "\n",
    "        # put them in a list\n",
    "        r_tab = [r1, r2,]\n",
    "\n",
    "        # now copy input pages to output\n",
    "        corr_page = corrections_doc.new_page(-1,\n",
    "                                        width = width,\n",
    "                                        height = height)\n",
    "        # insert input page into the correct rectangle\n",
    "        corr_page.show_pdf_page(r_tab[0],  # select output rect\n",
    "                           old_doc,  # input document\n",
    "                           0)  # input page number\n",
    "        corr_page.show_pdf_page(r_tab[1],  # select output rect\n",
    "                           new_doc,  # input document\n",
    "                           0)  # input page number\n",
    "\n",
    "        quads = corr_page.search_for(\"Equipment\", quads=False)\n",
    "        annot = corr_page.add_highlight_annot(quads)  # highlight it\n",
    "        annot.set_colors(stroke=pymupdf.pdfcolor[\"red\"])  # change default color\n",
    "        annot.update()\n",
    "\n",
    "        # TODO get positions of corrections to highlight, then modify positions to place them in the correct location on corr_page\n",
    "        # Then use pymupdf.Point(x, y) and pymupdf.Rect(Point0, Point1) to set position\n",
    "        # Then add_highlight_annot\n",
    "\n",
    "        # page = new_doc.load_page(page_idx)\n",
    "        # # quads = page.search_for(\"Equipment\", quads=True)\n",
    "        # annot = page.add_highlight_annot(quads)  # highlight it\n",
    "        # annot.set_colors(stroke=pymupdf.pdfcolor[\"red\"])  # change default color\n",
    "        # annot.update()\n",
    "\n",
    "        # old_page = old_doc.load_page(page_idx)\n",
    "        # old_annot = old_page.add_highlight_annot(quads)  # highlight it\n",
    "        # old_annot.set_colors(stroke=pymupdf.pdfcolor[\"red\"])  # change default color\n",
    "        # old_annot.update()\n",
    "\n",
    "        # by all means, save new file using garbage collection and compression\n",
    "        corrections_doc.save(image_path, garbage=3, deflate=True)\n",
    "\n",
    "    def annotate_errors(self,):\n",
    "        qa_folder = self.old_version.ocr_filepath.parent / 'QA'\n",
    "        os.makedirs(qa_folder, exist_ok=True)\n",
    "        # use pymupdf annotations to put rectangles in correct locations with a fill color\n",
    "        old_doc = pymupdf.open(self.old_version.orig_filepath)\n",
    "        new_doc = pymupdf.open(self.new_version.orig_filepath)\n",
    "        \n",
    "        image_path = qa_folder / 'test.pdf'\n",
    "        page_idx = 0\n",
    "        rect = [200, 300, 200, 300] \n",
    "        self.add_annotation_and_save_image(new_doc, old_doc, image_path, page_idx, rect, fill_color=(1,1,1))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_comp.old_version.sections\n",
    "# for rIdx, row_data in new_exhibit.sections['equipment specifications']['data'].iterrows():\n",
    "#     for val in row_data.index:\n",
    "#         print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ocr_pdf_path  = Path('amendments/New_Exhibit_Redacted_docTR.pdf')\n",
    "new_orig_pdf_path = Path('amendments/New_Exhibit_Redacted.pdf')\n",
    "old_ocr_pdf_path  = Path('amendments/Old_Exhibit_Redacted_docTR.pdf')\n",
    "old_orig_pdf_path = Path('amendments/Old_Exhibit_Redacted.pdf')\n",
    "\n",
    "config_path = Path(r'C:\\Users\\Dalton\\Documents\\personal_records\\apex_consulting\\doctr_ocr\\configs\\atc_extra_info_config.yaml')\n",
    "\n",
    "det_arch_options  = ['linknet_resnet18',\n",
    "                     'linknet_resnet34',\n",
    "                     'linknet_resnet50',\n",
    "                     'db_resnet50',\n",
    "                     'db_mobilenet_v3_large',\n",
    "                     'fast_tiny',\n",
    "                     'fast_small',\n",
    "                     'fast_base',]\n",
    "\n",
    "reco_arch_options = ['crnn_vgg16_bn',\n",
    "                     'crnn_mobilenet_v3_small',\n",
    "                     'crnn_mobilenet_v3_large',\n",
    "                     'sar_resnet31',\n",
    "                     'master',\n",
    "                     'vitstr_small',\n",
    "                     'vitstr_base',\n",
    "                     'parseq',]\n",
    "\n",
    "predictor = ocr_predictor(det_arch='fast_base', reco_arch='crnn_vgg16_bn', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exhibit = ATC_amendment(new_orig_pdf_path, new_ocr_pdf_path, config_path, ':')\n",
    "new_exhibit.get_text_lines_from_original()\n",
    "new_exhibit.run_ocr(predictor)\n",
    "new_exhibit.get_text_lines_from_ocr()\n",
    "new_exhibit.get_section_bounds()\n",
    "new_exhibit.fill_implicit_keys('ground space requirements', left_mult=2, right_mult=2)\n",
    "new_exhibit.extract_table_data()\n",
    "new_exhibit.extract_text_data()\n",
    "new_exhibit.align_line_configuration_data(group_key = 'line configuration'   , max_line_types=2)\n",
    "new_exhibit.align_line_configuration_data(group_key = 'conduit configuration', max_line_types=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                                                         RRU/RRH\n",
       "Manufacturer                                                                Ericsson\n",
       "Model #                                                                        RRU22\n",
       "Dimensions HxWxD                                                20.2\" x 13.2\" x 6.9\"\n",
       "Weight (lbs.)                                                                   52.9\n",
       "Location                                                                       Tower\n",
       "RAD Center AGL                                                                180.0'\n",
       "Tip Height                                                                    180.8'\n",
       "Base Height                                                                   179.2'\n",
       "Mount Type                                                                  Side Arm\n",
       "Quantity                                                                           1\n",
       "Azimuths/Dir. of Radiation                                                        60\n",
       "Quant. Per Azimuth/Sector                                                          1\n",
       "TX/RX Frequency Units                                                            MHz\n",
       "TX Frequency                                                   2145-2155,1735-\\n1740\n",
       "RX Frequency                                                   1745-1755,2135-\\n2140\n",
       "Using Unlicensed Frequencies?                                                     No\n",
       "Antenna Gain                                                                     N/A\n",
       "Total # of Lines                                                                   2\n",
       "Individual Line Configuration                                                    N/A\n",
       "Conduit Configuration              Qty: 1\\nType: 2\" conduit\\ncontaining:\\n-;\\nAzi...\n",
       "line_config_0_Qty                                                               None\n",
       "line_config_0_Type                                                              None\n",
       "line_config_0_Diameter                                                          None\n",
       "line_config_0_Azimuth/Sector                                                    None\n",
       "line_config_1_Qty                                                               None\n",
       "line_config_1_Type                                                              None\n",
       "line_config_1_Diameter                                                          None\n",
       "line_config_1_Azimuth/Sector                                                    None\n",
       "conduit_config_0_Qty                                                               1\n",
       "conduit_config_0_Type                                                     2\" conduit\n",
       "conduit_config_0_containing                                                       -;\n",
       "conduit_config_0_Azimuth/Sector                                                    1\n",
       "conduit_config_1_Qty                                                               1\n",
       "conduit_config_1_Type                                                     2\" conduit\n",
       "conduit_config_1_containing                                                       -;\n",
       "conduit_config_1_Azimuth/Sector                                                    1\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_exhibit.sections['ground space requirements']['data']\n",
    "new_exhibit.sections['backup power requirements']['data']\n",
    "new_exhibit.sections['utility requirements']['data']\n",
    "new_exhibit.sections['transmitter & receiver specifications']['data']\n",
    "new_exhibit.sections['equipment specifications']['data'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_exhibit = ATC_amendment(old_orig_pdf_path, old_ocr_pdf_path, config_path, ':')\n",
    "old_exhibit.get_text_lines_from_original()\n",
    "old_exhibit.run_ocr(predictor)\n",
    "old_exhibit.get_text_lines_from_ocr()\n",
    "old_exhibit.get_section_bounds()\n",
    "old_exhibit.fill_implicit_keys('ground space requirements', left_mult=2, right_mult=2)\n",
    "old_exhibit.extract_table_data()\n",
    "old_exhibit.extract_text_data()\n",
    "old_exhibit.align_line_configuration_data(group_key = 'separated configuration', max_line_types=2, type_key = 'line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                         data_not_found\n",
       "Type                                            rru/rrh\n",
       "Manufacturer                                   ericsson\n",
       "Model #                                           rru22\n",
       "Dimensions HxWxD                     20.2\" x 13.2\"x6.9\"\n",
       "Weight(lbs.)                                       52.9\n",
       "Location                                          tower\n",
       "RAD Center AGL                                   180.0'\n",
       "Equipment Tip Height                             180.8\"\n",
       "Equipment Base Height                            179.2\"\n",
       "Mount Type                                     side arm\n",
       "Quantity                                              1\n",
       "Azimuths/Dir. of Radiation                           60\n",
       "Quant. Per Azimuth/Sector                             1\n",
       "TX/RX Frequency Units                               mhz\n",
       "TX Frequency                       2145-2155,1735- 1740\n",
       "RX Frequency                       1745-1755,2135- 2140\n",
       "Using Unlicensed Frequencies?                        no\n",
       "Equipment Gain                                      n/a\n",
       "Total # of Lines                                      2\n",
       "Line Quant. Per Azimuth/Sector                        2\n",
       "Line Type                                       conduit\n",
       "Line Diameter Size                           2\" conduit\n",
       "Line Configuration                                  nia\n",
       "line_config_0_Qty                                  None\n",
       "line_config_0_Type                                 None\n",
       "line_config_0_Diameter                             None\n",
       "line_config_0_Azimuth/Sector                       None\n",
       "conduit_config_0_Qty                                  2\n",
       "conduit_config_0_containing                          -;\n",
       "conduit_config_0_Type                        2\" conduit\n",
       "conduit_config_0_Azimuth/Sector                       2\n",
       "Name: 9, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_exhibit.sections['ground space requirements']['data']\n",
    "# old_exhibit.sections['backup power requirements']['data']\n",
    "# old_exhibit.sections['utility requirements']['data']\n",
    "# old_exhibit.sections['transmitter & receiver specifications']['data']\n",
    "old_exhibit.sections['equipment specifications']['data'].iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp = version_comparison(new_exhibit, old_exhibit)\n",
    "data_comp.compare_data()\n",
    "data_comp.annotate_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still need to check/correct unique data storage\n",
    "1) Add input argument for known replacements (\"Equipment Tip\", \"Equipment Tip Height\"), (\"Equipment Base\", \"Equipment Base Height\")\n",
    "2) Code to process known replacements\n",
    "3) Code to detect unique keys in old data (currently new data unique keys are found)\n",
    "4) Check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
